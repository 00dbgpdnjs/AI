{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgnZOimXdN2V"
   },
   "source": [
    "#### **Attention is All You Need (NIPS 2017)** 실습\n",
    "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPTu1gCK1YXd"
   },
   "source": [
    "#### <b>BLEU Score 계산을 위한 라이브러리 업데이트</b>\n",
    "* 이를 사용하기 위해 torchtext==0.6.0 버전으로 설치\n",
    "* <b>[Restart Runtime]</b> 버튼을 눌러 런타임을 재시작할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7gjy4bZ1aXc",
    "outputId": "efe343fc-fcf4-4dc3-dd2e-91af526844b9"
   },
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 777\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V6NaGYTd62g"
   },
   "source": [
    "#### **데이터 전처리(Preprocessing)**\n",
    "\n",
    "* **spaCy 라이브러리**: 문장의 토큰화(tokenization), 태깅(tagging) 등의 전처리 기능을 위한 라이브러리\n",
    "  * 영어(Engilsh)와 독일어(Deutsch) 전처리 모듈 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "tbQzI6V1a2m_"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#   # 출력을 바로 아래에 표시되지 않고 %%capture captured_output 면\n",
    "#   # captured_output을 접근하여 캡처된 출력을 프로그래밍적으로 확인하거나 처리\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfOdc9FSd7xt",
    "outputId": "1bc22050-8cab-4f59-913c-0315d45f90d0"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eet4iWy_d8s7",
    "outputId": "1547eadd-1ba7-42f6-813c-5c6030866547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am a graduate student."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 간단히 토큰화(tokenization) 기능 써보기\n",
    "spacy_en.tokenizer(\"I am a graduate student.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqhzmLvjeFZE"
   },
   "source": [
    "* 영어(English) 및 독일어(Deutsch) **토큰화 함수** 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "\n",
    "bert_model_name = 'camembert-base'\n",
    "# bert_model_name = 'camembert/camembert-large'\n",
    "tokenizer = CamembertTokenizer.from_pretrained(bert_model_name) # local_files_only=True)\n",
    "bert_model = CamembertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# camembert.eval()  # disable dropout (or leave in train mode to finetune)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "### camembert-base 크게 세 가지\n",
    "# bert_model.embeddings \n",
    "# bert_model.encoder\n",
    "# bert_model.pooler\n",
    "# bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in bert_model.parameters():\n",
    "# for param in bert_model.embeddings.parameters():\n",
    "for name, param in bert_model.named_parameters():\n",
    "    # print(name)\n",
    "    if 'encoder.layer.11.output' not in name and 'pooler' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight                  False\n",
      "embeddings.position_embeddings.weight              False\n",
      "embeddings.token_type_embeddings.weight            False\n",
      "embeddings.LayerNorm.weight                        False\n",
      "embeddings.LayerNorm.bias                          False\n",
      "encoder.layer.0.attention.self.query.weight        False\n",
      "encoder.layer.0.attention.self.query.bias          False\n",
      "encoder.layer.0.attention.self.key.weight          False\n",
      "encoder.layer.0.attention.self.key.bias            False\n",
      "encoder.layer.0.attention.self.value.weight        False\n",
      "encoder.layer.0.attention.self.value.bias          False\n",
      "encoder.layer.0.attention.output.dense.weight      False\n",
      "encoder.layer.0.attention.output.dense.bias        False\n",
      "encoder.layer.0.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.0.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.0.intermediate.dense.weight          False\n",
      "encoder.layer.0.intermediate.dense.bias            False\n",
      "encoder.layer.0.output.dense.weight                False\n",
      "encoder.layer.0.output.dense.bias                  False\n",
      "encoder.layer.0.output.LayerNorm.weight            False\n",
      "encoder.layer.0.output.LayerNorm.bias              False\n",
      "encoder.layer.1.attention.self.query.weight        False\n",
      "encoder.layer.1.attention.self.query.bias          False\n",
      "encoder.layer.1.attention.self.key.weight          False\n",
      "encoder.layer.1.attention.self.key.bias            False\n",
      "encoder.layer.1.attention.self.value.weight        False\n",
      "encoder.layer.1.attention.self.value.bias          False\n",
      "encoder.layer.1.attention.output.dense.weight      False\n",
      "encoder.layer.1.attention.output.dense.bias        False\n",
      "encoder.layer.1.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.1.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.1.intermediate.dense.weight          False\n",
      "encoder.layer.1.intermediate.dense.bias            False\n",
      "encoder.layer.1.output.dense.weight                False\n",
      "encoder.layer.1.output.dense.bias                  False\n",
      "encoder.layer.1.output.LayerNorm.weight            False\n",
      "encoder.layer.1.output.LayerNorm.bias              False\n",
      "encoder.layer.2.attention.self.query.weight        False\n",
      "encoder.layer.2.attention.self.query.bias          False\n",
      "encoder.layer.2.attention.self.key.weight          False\n",
      "encoder.layer.2.attention.self.key.bias            False\n",
      "encoder.layer.2.attention.self.value.weight        False\n",
      "encoder.layer.2.attention.self.value.bias          False\n",
      "encoder.layer.2.attention.output.dense.weight      False\n",
      "encoder.layer.2.attention.output.dense.bias        False\n",
      "encoder.layer.2.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.2.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.2.intermediate.dense.weight          False\n",
      "encoder.layer.2.intermediate.dense.bias            False\n",
      "encoder.layer.2.output.dense.weight                False\n",
      "encoder.layer.2.output.dense.bias                  False\n",
      "encoder.layer.2.output.LayerNorm.weight            False\n",
      "encoder.layer.2.output.LayerNorm.bias              False\n",
      "encoder.layer.3.attention.self.query.weight        False\n",
      "encoder.layer.3.attention.self.query.bias          False\n",
      "encoder.layer.3.attention.self.key.weight          False\n",
      "encoder.layer.3.attention.self.key.bias            False\n",
      "encoder.layer.3.attention.self.value.weight        False\n",
      "encoder.layer.3.attention.self.value.bias          False\n",
      "encoder.layer.3.attention.output.dense.weight      False\n",
      "encoder.layer.3.attention.output.dense.bias        False\n",
      "encoder.layer.3.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.3.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.3.intermediate.dense.weight          False\n",
      "encoder.layer.3.intermediate.dense.bias            False\n",
      "encoder.layer.3.output.dense.weight                False\n",
      "encoder.layer.3.output.dense.bias                  False\n",
      "encoder.layer.3.output.LayerNorm.weight            False\n",
      "encoder.layer.3.output.LayerNorm.bias              False\n",
      "encoder.layer.4.attention.self.query.weight        False\n",
      "encoder.layer.4.attention.self.query.bias          False\n",
      "encoder.layer.4.attention.self.key.weight          False\n",
      "encoder.layer.4.attention.self.key.bias            False\n",
      "encoder.layer.4.attention.self.value.weight        False\n",
      "encoder.layer.4.attention.self.value.bias          False\n",
      "encoder.layer.4.attention.output.dense.weight      False\n",
      "encoder.layer.4.attention.output.dense.bias        False\n",
      "encoder.layer.4.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.4.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.4.intermediate.dense.weight          False\n",
      "encoder.layer.4.intermediate.dense.bias            False\n",
      "encoder.layer.4.output.dense.weight                False\n",
      "encoder.layer.4.output.dense.bias                  False\n",
      "encoder.layer.4.output.LayerNorm.weight            False\n",
      "encoder.layer.4.output.LayerNorm.bias              False\n",
      "encoder.layer.5.attention.self.query.weight        False\n",
      "encoder.layer.5.attention.self.query.bias          False\n",
      "encoder.layer.5.attention.self.key.weight          False\n",
      "encoder.layer.5.attention.self.key.bias            False\n",
      "encoder.layer.5.attention.self.value.weight        False\n",
      "encoder.layer.5.attention.self.value.bias          False\n",
      "encoder.layer.5.attention.output.dense.weight      False\n",
      "encoder.layer.5.attention.output.dense.bias        False\n",
      "encoder.layer.5.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.5.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.5.intermediate.dense.weight          False\n",
      "encoder.layer.5.intermediate.dense.bias            False\n",
      "encoder.layer.5.output.dense.weight                False\n",
      "encoder.layer.5.output.dense.bias                  False\n",
      "encoder.layer.5.output.LayerNorm.weight            False\n",
      "encoder.layer.5.output.LayerNorm.bias              False\n",
      "encoder.layer.6.attention.self.query.weight        False\n",
      "encoder.layer.6.attention.self.query.bias          False\n",
      "encoder.layer.6.attention.self.key.weight          False\n",
      "encoder.layer.6.attention.self.key.bias            False\n",
      "encoder.layer.6.attention.self.value.weight        False\n",
      "encoder.layer.6.attention.self.value.bias          False\n",
      "encoder.layer.6.attention.output.dense.weight      False\n",
      "encoder.layer.6.attention.output.dense.bias        False\n",
      "encoder.layer.6.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.6.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.6.intermediate.dense.weight          False\n",
      "encoder.layer.6.intermediate.dense.bias            False\n",
      "encoder.layer.6.output.dense.weight                False\n",
      "encoder.layer.6.output.dense.bias                  False\n",
      "encoder.layer.6.output.LayerNorm.weight            False\n",
      "encoder.layer.6.output.LayerNorm.bias              False\n",
      "encoder.layer.7.attention.self.query.weight        False\n",
      "encoder.layer.7.attention.self.query.bias          False\n",
      "encoder.layer.7.attention.self.key.weight          False\n",
      "encoder.layer.7.attention.self.key.bias            False\n",
      "encoder.layer.7.attention.self.value.weight        False\n",
      "encoder.layer.7.attention.self.value.bias          False\n",
      "encoder.layer.7.attention.output.dense.weight      False\n",
      "encoder.layer.7.attention.output.dense.bias        False\n",
      "encoder.layer.7.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.7.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.7.intermediate.dense.weight          False\n",
      "encoder.layer.7.intermediate.dense.bias            False\n",
      "encoder.layer.7.output.dense.weight                False\n",
      "encoder.layer.7.output.dense.bias                  False\n",
      "encoder.layer.7.output.LayerNorm.weight            False\n",
      "encoder.layer.7.output.LayerNorm.bias              False\n",
      "encoder.layer.8.attention.self.query.weight        False\n",
      "encoder.layer.8.attention.self.query.bias          False\n",
      "encoder.layer.8.attention.self.key.weight          False\n",
      "encoder.layer.8.attention.self.key.bias            False\n",
      "encoder.layer.8.attention.self.value.weight        False\n",
      "encoder.layer.8.attention.self.value.bias          False\n",
      "encoder.layer.8.attention.output.dense.weight      False\n",
      "encoder.layer.8.attention.output.dense.bias        False\n",
      "encoder.layer.8.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.8.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.8.intermediate.dense.weight          False\n",
      "encoder.layer.8.intermediate.dense.bias            False\n",
      "encoder.layer.8.output.dense.weight                False\n",
      "encoder.layer.8.output.dense.bias                  False\n",
      "encoder.layer.8.output.LayerNorm.weight            False\n",
      "encoder.layer.8.output.LayerNorm.bias              False\n",
      "encoder.layer.9.attention.self.query.weight        False\n",
      "encoder.layer.9.attention.self.query.bias          False\n",
      "encoder.layer.9.attention.self.key.weight          False\n",
      "encoder.layer.9.attention.self.key.bias            False\n",
      "encoder.layer.9.attention.self.value.weight        False\n",
      "encoder.layer.9.attention.self.value.bias          False\n",
      "encoder.layer.9.attention.output.dense.weight      False\n",
      "encoder.layer.9.attention.output.dense.bias        False\n",
      "encoder.layer.9.attention.output.LayerNorm.weight  False\n",
      "encoder.layer.9.attention.output.LayerNorm.bias    False\n",
      "encoder.layer.9.intermediate.dense.weight          False\n",
      "encoder.layer.9.intermediate.dense.bias            False\n",
      "encoder.layer.9.output.dense.weight                False\n",
      "encoder.layer.9.output.dense.bias                  False\n",
      "encoder.layer.9.output.LayerNorm.weight            False\n",
      "encoder.layer.9.output.LayerNorm.bias              False\n",
      "encoder.layer.10.attention.self.query.weight       False\n",
      "encoder.layer.10.attention.self.query.bias         False\n",
      "encoder.layer.10.attention.self.key.weight         False\n",
      "encoder.layer.10.attention.self.key.bias           False\n",
      "encoder.layer.10.attention.self.value.weight       False\n",
      "encoder.layer.10.attention.self.value.bias         False\n",
      "encoder.layer.10.attention.output.dense.weight     False\n",
      "encoder.layer.10.attention.output.dense.bias       False\n",
      "encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "encoder.layer.10.attention.output.LayerNorm.bias   False\n",
      "encoder.layer.10.intermediate.dense.weight         False\n",
      "encoder.layer.10.intermediate.dense.bias           False\n",
      "encoder.layer.10.output.dense.weight               False\n",
      "encoder.layer.10.output.dense.bias                 False\n",
      "encoder.layer.10.output.LayerNorm.weight           False\n",
      "encoder.layer.10.output.LayerNorm.bias             False\n",
      "encoder.layer.11.attention.self.query.weight       False\n",
      "encoder.layer.11.attention.self.query.bias         False\n",
      "encoder.layer.11.attention.self.key.weight         False\n",
      "encoder.layer.11.attention.self.key.bias           False\n",
      "encoder.layer.11.attention.self.value.weight       False\n",
      "encoder.layer.11.attention.self.value.bias         False\n",
      "encoder.layer.11.attention.output.dense.weight     False\n",
      "encoder.layer.11.attention.output.dense.bias       False\n",
      "encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "encoder.layer.11.attention.output.LayerNorm.bias   False\n",
      "encoder.layer.11.intermediate.dense.weight         False\n",
      "encoder.layer.11.intermediate.dense.bias           False\n",
      "encoder.layer.11.output.dense.weight               True\n",
      "encoder.layer.11.output.dense.bias                 True\n",
      "encoder.layer.11.output.LayerNorm.weight           True\n",
      "encoder.layer.11.output.LayerNorm.bias             True\n",
      "pooler.dense.weight                                True\n",
      "pooler.dense.bias                                  True\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert_model.named_parameters():\n",
    "    # <는 왼쪽 정렬, 숫자는 해당 필드의 최소 폭\n",
    "    print(\"{:<50} {}\".format(name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "USWSV869d-s7"
   },
   "outputs": [],
   "source": [
    "# 독일어(Deutsch) 문장을 토큰화 하는 함수 (순서를 뒤집지 않음)\n",
    "def tokenize_de(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "# 영어(English) 문장을 토큰화 하는 함수\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYi1dM7-eH0N"
   },
   "source": [
    "* **필드(field)** 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시합니다.\n",
    "    * init_token=\"<sos>\" : 번역 모델의 입력으로 넣을 때 각각의 문장들의 앞부분에는 sos 토큰을 붙임\n",
    "    * lower : 각 단어를 소문자로 바꾸는 것이 일반적\n",
    "    * batch_first : 트랜스포머의 입력을 넣을 때는 텐서의 차원에서 시퀀스 보다 배치가 먼저 오도록\n",
    "* Seq2Seq 모델과는 다르게 <b>batch_first 속성의 값을 True로 설정</b>합니다.\n",
    "* 번역 목표\n",
    "    * 소스(SRC): 독일어\n",
    "    * 목표(TRG): 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "r_dSDRtReGnU"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(\n",
    "    use_vocab=False,\n",
    "    tokenize=tokenize_de,\n",
    "    pad_token=tokenizer.pad_token_id,  # 패딩 토큰 ID 설정 @단어 집합의 문자열 부분도 숫자로 들어감 \n",
    "    init_token=tokenizer.cls_token_id,  # 시퀀스 시작 토큰 ID 설정\n",
    "    eos_token=tokenizer.sep_token_id,  # 문장 종료 토큰 ID 설정\n",
    "    unk_token=tokenizer.unk_token_id,  # 알 수 없는 토큰 ID 설정\n",
    "    batch_first=True\n",
    ")\n",
    "TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX0O1oKQeY2y"
   },
   "source": [
    "* 대표적인 영어-독어 번역 데이터셋인 **Multi30k**를 불러옵니다.\n",
    "    * 약 3만개의 영어 독일어 쌍을 가짐\n",
    "    * fields=(SRC, TRG) : 위 Field 라이버르러리를 이용해서 독일어를 영어로 바꾸는 태스크에 대해서 각각 앞서 정의했던 전처리(SRC, TRG)를 수행할 수 있도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/data/hwyu/data/libri/'\n",
    "\n",
    "train_en_path = filepath+\"train/train.en\"\n",
    "train_de_path = filepath+\"train/train_gtranslate.fr\" \n",
    "\n",
    "train_oth_en_path = filepath+\"train/other.en\"\n",
    "train_oth_de_path = filepath+\"train/other_gtranslate.fr\" \n",
    "\n",
    "test_eng_path = filepath+\"test/test.en\"\n",
    "test_de_path = filepath+\"test/test_gtranslate.fr\"\n",
    "\n",
    "test_dev_eng_path = filepath+\"test/dev.en\"\n",
    "test_dev_de_path = filepath+\"test/dev_gtranslate.fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108640\n",
      "['ADIEU VALENTINE ADIEU', 'PROVE IT DANGLARS', 'SAID FRANZ', 'SAID FRANZ', 'FERNAND MONDEGO', 'AND THE CORRIDOR', 'FOR ANDREA RONDOLO', 'FOR ANDREA RONDOLO', 'SAID FERNAND', 'SAID ANDREA']\n"
     ]
    }
   ],
   "source": [
    "end_line = 100000  # 끝 라인 (포함하지 않음)\n",
    "\n",
    "train_en_sub_raw = []  # 추출한 데이터를 저장할 리스트\n",
    "\n",
    "with open(train_en_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        train_en_sub_raw.append(line.strip())\n",
    "        if i + 1 >= end_line:\n",
    "            break\n",
    "\n",
    "with open(train_oth_en_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        train_en_sub_raw.append(line.strip())\n",
    "        if i + 1 >= end_line:\n",
    "            break\n",
    "            \n",
    "limit_data_len = 120000\n",
    "train_en_sub_raw = train_en_sub_raw[:limit_data_len]\n",
    "# 데이터 확인\n",
    "print(len(train_en_sub_raw))\n",
    "print(train_en_sub_raw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108640\n",
      "['\"Adieu, Valentine, adieux!', 'Prouve, Danglars.', 'Dit Franz.', 'Dit Franz.', '\"\" Fernand Mondego.', '\"\" Et le couloir?', '\"\" Pour Andrea Rondolo?', '\"\" Pour Andrea Rondolo?', '\"A déclaré Fernand.', '\"Dit Andrea.']\n"
     ]
    }
   ],
   "source": [
    "train_fr_sub_raw = []\n",
    "\n",
    "with open(train_de_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        train_fr_sub_raw.append(line.strip())\n",
    "        if i + 1 >= end_line:\n",
    "            break\n",
    "\n",
    "with open(train_oth_de_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        train_fr_sub_raw.append(line.strip())\n",
    "        if i + 1 >= end_line:\n",
    "            break\n",
    "train_fr_sub_raw = train_fr_sub_raw[:limit_data_len]\n",
    "# 데이터 확인\n",
    "print(len(train_fr_sub_raw))\n",
    "print(train_fr_sub_raw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108639\n",
      "108639\n",
      "('ADIEU VALENTINE ADIEU', 'PROVE IT DANGLARS', 'SAID FRANZ', 'SAID FRANZ', 'FERNAND MONDEGO', 'AND THE CORRIDOR', 'FOR ANDREA RONDOLO', 'FOR ANDREA RONDOLO', 'SAID FERNAND', 'SAID ANDREA')\n",
      "('\"Adieu, Valentine, adieux!', 'Prouve, Danglars.', 'Dit Franz.', 'Dit Franz.', '\"\" Fernand Mondego.', '\"\" Et le couloir?', '\"\" Pour Andrea Rondolo?', '\"\" Pour Andrea Rondolo?', '\"A déclaré Fernand.', '\"Dit Andrea.')\n"
     ]
    }
   ],
   "source": [
    "def filter_long_sentences(french_data, english_data, max_french_length):\n",
    "    filtered_data = []\n",
    "    for fr, en in zip(french_data, english_data):\n",
    "        if len(fr.split()) <= max_french_length:\n",
    "            filtered_data.append((fr, en))\n",
    "    return filtered_data\n",
    "\n",
    "# 최대 프랑스어 문장 길이\n",
    "max_french_length = 350\n",
    "\n",
    "# 문장 길이가 최대값을 초과하는 문장 제거\n",
    "filtered_data = filter_long_sentences(train_fr_sub_raw, train_en_sub_raw, max_french_length)\n",
    "\n",
    "# 새로운 데이터셋 생성\n",
    "train_fr_sub_raw, train_en_sub_raw = zip(*filtered_data)\n",
    "\n",
    "print(len(train_en_sub_raw))\n",
    "print(len(train_fr_sub_raw))\n",
    "print(train_en_sub_raw[:10])\n",
    "print(train_fr_sub_raw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IN ANOTHER THE GROUND WAS CUMBERED WITH RUSTY IRON MONSTERS OF STEAM BOILERS WHEELS CRANKS PIPES FURNACES PADDLES ANCHORS DIVING BELLS WINDMILL SAILS AND I KNOW NOT WHAT STRANGE OBJECTS ACCUMULATED BY SOME SPECULATOR AND GROVELLING IN THE DUST UNDERNEATH WHICH HAVING SUNK INTO THE SOIL OF THEIR OWN WEIGHT IN WET WEATHER THEY HAD THE APPEARANCE OF VAINLY TRYING TO HIDE THEMSELVES\\n', 'THE CLASH AND GLARE OF SUNDRY FIERY WORKS UPON THE RIVER SIDE AROSE BY NIGHT TO DISTURB EVERYTHING EXCEPT THE HEAVY AND UNBROKEN SMOKE THAT POURED OUT OF THEIR CHIMNEYS\\n']\n",
      "[\"Dans un autre, le terrain était encombré de monstres en fer rouillé de chaudières à vapeur, de roues, de manivelles, de tuyaux, de fours, de pagaies, d'ancrages, de cloches, de voiliers, et je ne sais pas quels objets étranges, accumulés par un spéculateur, et Se creusant dans la poussière, sous laquelle - s'étant enfoncée dans le sol de leur propre poids par temps humide - ils avaient l'apparence d'essayer vainement de se cacher.\\n\", \"Le choc et l'éblouissement de divers travaux de feu sur le côté de la rivière, se sont produits de nuit pour déranger tout sauf la fumée lourde et ininterrompue qui a répandu leurs cheminées.\\n\"]\n",
      "2048\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "with open(test_eng_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_en_raw = file.readlines()\n",
    "    \n",
    "with open(test_de_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_fr_raw = file.readlines()\n",
    "\n",
    "with open(test_dev_eng_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_dev_en_raw = file.readlines()\n",
    "    \n",
    "with open(test_dev_de_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    test_dev_fr_raw = file.readlines()\n",
    "\n",
    "print(test_en_raw[:2])\n",
    "print(test_fr_raw[:2])\n",
    "\n",
    "print(len(test_en_raw))\n",
    "print(len(test_fr_raw))\n",
    "test_en_raw += test_dev_en_raw\n",
    "test_fr_raw += test_dev_fr_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return normalize('NFD', s).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, Beauchamp, Beauchamp, comment puis-je aborder le mien?\n",
      "Oh Beauchamp Beauchamp comment puis je aborder le mien\n",
      "Mais, après m'avoir péché, peut-être plus profondément que d'autres, je ne me reposerai jamais avant d'avoir déchiré les semblables de mes semblables et j'ai découvert leurs faiblesses. Je les ai toujours trouvés; Et plus encore, je le répète avec joie, avec triomphe, j'ai toujours trouvé une preuve de perversité humaine ou d'erreur.\n",
      "Mais apres m avoir peche peut etre plus profondement que d autres je ne me reposerai jamais avant d avoir dechire les semblables de mes semblables et j ai decouvert leurs faiblesses Je les ai toujours trouves Et plus encore je le repete avec joie avec triomphe j ai toujours trouve une preuve de perversite humaine ou d erreur\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "\n",
    "def normalizeString(s):\n",
    "    # s = unicodeToAscii(s.lower().strip())\n",
    "    s = unicodeToAscii(s)\n",
    "    # s = re.sub(r\"([.!?\\\"])\", r\" \\1\", s) # 마침표, 느낌표, 물음표, 따옴표 앞에 공백을 추가\n",
    "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) # G 영문 알파벳과 마침표(.), 느낌표(!), 물음표(?)를 제외한 모든 문자를 공백으로 대체/ +: 해당 패턴이 한 번 이상 반복\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s) # 이 코드 실행 후 strip() 안하면 문장 앞뒤 제거됐을 때 공백생김 \n",
    "    return s.strip()\n",
    "\n",
    "print(train_fr_sub_raw[2222]) \n",
    "print(normalizeString(train_fr_sub_raw[2222]))\n",
    "\n",
    "print(train_fr_sub_raw[3332]) \n",
    "print(normalizeString(train_fr_sub_raw[3332]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeStrings(lines):\n",
    "    return [normalizeString(s) for s in lines]\n",
    "    \n",
    "train_fr_sub = normalizeStrings(train_fr_sub_raw)\n",
    "train_en_sub = normalizeStrings(train_en_sub_raw)\n",
    "test_fr = normalizeStrings(test_fr_raw)\n",
    "test_en = normalizeStrings(test_en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Le celebre Cucumetto poursuivi dans les Abruzzes chasse du royaume de Naples ou il avait mene une guerre reguliere avait traverse le Garigliano comme Manfred et s etait refugie sur les rives de l Amasine entre Sonnino et Juperno', 'Vous connaissez les environs de Paris alors', 'Vous connaissez les environs de Paris alors', 'Franz prit la lampe et entra dans la grotte souterraine suivie de Gaetano', 'Gaetano ne se trompait pas']\n",
      "('\"Le célèbre Cucumetto, poursuivi dans les Abruzzes, chassé du royaume de Naples, où il avait mené une guerre régulière, avait traversé le Garigliano, comme Manfred, et s\\'était réfugié sur les rives de l\\'Amasine entre Sonnino et Juperno.', '\"\" Vous connaissez les environs de Paris, alors?', '\"\" Vous connaissez les environs de Paris, alors?', 'Franz prit la lampe et entra dans la grotte souterraine, suivie de Gaetano.', 'Gaetano ne se trompait pas.')\n"
     ]
    }
   ],
   "source": [
    "print(train_fr_sub[100:105])\n",
    "print(train_fr_sub_raw[100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset, Example\n",
    "\n",
    "def create_dataset(text_list, field_name, field_obj):\n",
    "    # G Example.fromlist(data_list, fields) : field 정의에 따라 데이터를 적절히 처리하고 Example 객체를 생성\n",
    "    #   data_list의 순서는 필드 정의에서 지정한 순서와 일치해야 합니다.\n",
    "    examples = [Example.fromlist([text], [(field_name, field_obj)]) for text in text_list]\n",
    "    return Dataset(examples, fields=[(field_name, field_obj)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리브리스피치는 아래 부분 7_mydata로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText, GloVe\n",
    "\n",
    "flag_pretrained_emb = False\n",
    "\n",
    "train_data = create_dataset(train_fr_sub, 'src', SRC)\n",
    "\n",
    "train_data.fields['trg'] = TRG\n",
    "train_en_data = create_dataset(train_en_sub, 'trg', TRG)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i].trg = train_en_data[i].trg\n",
    "\n",
    "test_data = create_dataset(test_fr, 'src', SRC)\n",
    "\n",
    "test_data.fields['trg'] = TRG\n",
    "test_en_path = create_dataset(test_en, 'trg', TRG)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i].trg = test_en_path[i].trg\n",
    "\n",
    "max_vocab_size = 100000 # 12000\n",
    "if flag_pretrained_emb :\n",
    "    # print(\"loading en_pretrained_emb. . .\")\n",
    "    # en_pretrained_emb = GloVe(name='6B', dim=300, cache='/data/hwyu/1_seq2seq/cache_dir_en')\n",
    "\n",
    "    print(\"loading en_fasttext. . .\")\n",
    "    en_pretrained_emb = torch.load('/data/hwyu/1_seq2seq/en_fasttext_dim300.pt')\n",
    "    print(\"load 완료\\n\")\n",
    "\n",
    "    TRG.build_vocab(train_data, max_size=max_vocab_size, min_freq=2, vectors=en_pretrained_emb)\n",
    "else :\n",
    "    TRG.build_vocab(train_data, max_size=max_vocab_size, min_freq=2) # min_freq=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "KQ-Lhpp0ecOi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋(training dataset) 크기: 108639개\n",
      "테스트 데이터셋(testing dataset) 크기: 3119개\n"
     ]
    }
   ],
   "source": [
    "print(f\"학습 데이터셋(training dataset) 크기: {len(train_data.examples)}개\")\n",
    "# print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_data.examples)}개\")\n",
    "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_data.examples)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "VYghX0SueecT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137, 4428, 55, 24968]\n",
      "['asked', 'franz']\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 중 하나를 선택해 출력\n",
    "# ein ~ 문장이 들어 왔을 때 a ~ 이러한 영어 문장을 출력하도록 학습 데이터가 구성됨\n",
    "# G vars() : 내장 함수 중 하나로, 인자가 객체인 경우 해당 객체의 __dict__ 속성 반환\n",
    "    # __dict__ : 이 사전에는 객체가 가지고 있는 모든 속성과 해당 값 포함\n",
    "    # vars(obj)  # {'x': 10, 'y': 20}\n",
    "        # x, y : obj 인스턴스의 멤버변수\n",
    "    # 인자 없이 호출 : 현재 스코프에서 정의된 모든 변수와 그 값\n",
    "print(vars(train_data.examples[30])['src'])\n",
    "print(vars(train_data.examples[30])['trg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekQys1HpegX_"
   },
   "source": [
    "* **필드(field)** 객체의 **build_vocab** 메서드를 이용해 영어와 독어의 단어 사전을 생성합니다.\n",
    "  * 초기 input 디멘젼을 구하기 위해 \n",
    "  * **최소 2번 이상** 등장한 단어만을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "X4A5ksMyefKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 126315), ('and', 70406), ('of', 66454), ('to', 59299), ('a', 46152), ('in', 35949), ('i', 33435), ('he', 29094), ('that', 28312), ('was', 25626)]\n",
      "len(TRG): 29142\n"
     ]
    }
   ],
   "source": [
    "# SRC.build_vocab(train_dataset, min_freq=2)\n",
    "# TRG.build_vocab(train_data, min_freq=2)\n",
    "print(TRG.vocab.freqs.most_common(10))\n",
    "\n",
    "# print(f\"len(SRC): {len(SRC.vocab)}\") # 독일어는 7855개의 유의미한 단어가 있음\n",
    "print(f\"len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "xfo21o_5ehmK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "15039\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# stoi : 해당 단어의 인덱스 \n",
    "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0 ;unk\n",
    "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
    "print(TRG.vocab.stoi[\"<sos>\"]) # <sos>: 2\n",
    "print(TRG.vocab.stoi[\"<eos>\"]) # <eos>: 3\n",
    "print(TRG.vocab.stoi[\"hello\"]) # 단어의 인덱스 출력\n",
    "print(TRG.vocab.stoi[\"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eos> <sos> <pad> <unk>\n",
      "6 5 1 3\n"
     ]
    }
   ],
   "source": [
    "print(TRG.eos_token, TRG.init_token, TRG.pad_token, TRG.unk_token)\n",
    "print(SRC.eos_token, SRC.init_token, SRC.pad_token, SRC.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_pretrained_emb :\n",
    "    print(\"\\n임베딩 벡터 확인:\")\n",
    "    print(\"English Embedding Shape:\", TRG.vocab.vectors.shape)\n",
    "    \n",
    "    print(\"\\n임베딩 벡터 값 확인:\")\n",
    "    print(\"English Embedding Vectors:\")\n",
    "    print(TRG.vocab.vectors)\n",
    "    \n",
    "    # G 특별 토큰들에 대한 임베딩은 훈련되지 않습니다. \n",
    "    #  따라서 초기화 단계에서는 이러한 토큰들의 임베딩은 보통 0으로 설정됩니다.\n",
    "    # print(torch.all(TRG.vocab.vectors[0] == TRG.vocab.vectors[1]))\n",
    "    # print(torch.all(TRG.vocab.vectors[1] == TRG.vocab.vectors[2]))\n",
    "    # print(torch.all(TRG.vocab.vectors[2] == TRG.vocab.vectors[3]))\n",
    "    # print(torch.all(TRG.vocab.vectors[3] == TRG.vocab.vectors[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_pretrained_emb :\n",
    "    def count_zero_rows(matrix):\n",
    "        count = 0\n",
    "        for row in matrix:\n",
    "            if all(element == 0 for element in row):\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    print(\"0으로만 이루어진 행의 개수:\", count_zero_rows(TRG.vocab.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_pretrained_emb :\n",
    "    trg_embedding = torch.nn.Embedding.from_pretrained(TRG.vocab.vectors) # freeze=True # 디폴트\n",
    "    print(trg_embedding.weight.requires_grad)\n",
    "    \n",
    "    # 임베딩 레이어의 가중치 확인:\n",
    "    print(\"\\nEnglish Embedding Layer Weights:\")\n",
    "    print(trg_embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 108639\n",
      "Number of testing examples: 3119\n",
      "dict_keys(['src', 'trg'])\n",
      "txt 파일 : Et le couloir\n",
      "txt 파일 : AND THE CORRIDOR\n",
      "dict_values([[139, 16, 9144], ['and', 'the', 'corridor']])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "# print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "print(train_data[5].__dict__.keys())\n",
    "print(f\"txt 파일 : {train_fr_sub[5]}\")\n",
    "print(f\"txt 파일 : {train_en_sub[5]}\")\n",
    "# data 딕셔너리의 내용이 보기 좋게 출력\n",
    "pprint(train_data[5].__dict__.values()) # @나는 콤마 뒤는 잘림 .tok 파일은 다 자르나? 아니다  format='csv'로 해서 그런거였음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([[137, 4428, 55, 137, 7211, 105], ['asked', 'debray']])\n",
      "txt 파일 : Demanda Debray\n",
      "txt 파일 : ASKED DEBRAY\n"
     ]
    }
   ],
   "source": [
    "pprint(train_data[36].__dict__.values())\n",
    "print(f\"txt 파일 : {train_fr_sub[36]}\")\n",
    "print(f\"txt 파일 : {train_en_sub[36]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['src', 'trg'])\n",
      "dict_values([[51, 33, 18553], ['he', 'demanded']])\n",
      "txt 파일 : il a ordonne\n",
      "txt 파일 : HE DEMANDED\n"
     ]
    }
   ],
   "source": [
    "print(test_data[113].__dict__.keys())\n",
    "pprint(test_data[113].__dict__.values())\n",
    "print(f\"txt 파일 : {test_fr[113]}\")\n",
    "print(f\"txt 파일 : {test_en[113]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Demanda Debray']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode([[137, 4428, 55, 137, 7211, 105]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHlAcqrGekNm"
   },
   "source": [
    "* 한 문장에 포함된 단어가 순서대로 나열된 상태로 네트워크에 입력되어야 합니다.\n",
    "    * ~따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋습니다.~\n",
    "    * 이를 위해 BucketIterator를 사용합니다.\n",
    "      - 다른 길이의 시퀀스를 효율적으로 묶어서 배치하는 데이터 이터레이터의 한 유형\n",
    "      - 버킷 : 시퀀스를 길이별로 정렬하여 서로 다른 \"버킷\"이나 그룹으로 나누는 과정\n",
    "      - 훈련 중에는 이러한 버킷에서 시퀀스를 선택하여 배치를 형성\n",
    "    * **배치 크기(batch size)**: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "DSJQUC0meifi"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=BATCH_SIZE, # 디폴트 1\n",
    "    # sort : 데이터를 정렬할지 여부를 지정하는 인자, 기본값은 True\n",
    "    # sort_within_batch=True, # 기본값은 False / ?? 안해도 되나 -> sort 인자가 트루면 각 배치 내에서 데이터를 sort_key에 따라 다시 정렬\n",
    "    sort_key=lambda x: len(x.src), # G 유사한 길이의 문장을 같은 배치로 묶기 위해\n",
    "        # 안하면 evaluate() 실행시 TypeError: '<' not supported between instances of 'Example' and 'Example'\n",
    "        # 근데 왜 train() 실행 시에는 에러 안나는 거지 \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German -  114 13036 11407 35 22934 290  Length -  6\n",
      "English -  adieu valentine adieu  Length -  3\n",
      "\n",
      "German -  1092 8304 160 1907 5481 10  Length -  6\n",
      "English -  prove it danglars  Length -  3\n",
      "\n",
      "German -  21287 24968  Length -  2\n",
      "English -  said franz  Length -  2\n",
      "\n",
      "Maximum Length of English sentence 272 and German sentence 395 in the dataset\n",
      "Minimum Length of English sentence 1 and German sentence 0 in the dataset\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "max_len_eng = []\n",
    "max_len_ger = []\n",
    "for data in train_data:\n",
    "  max_len_ger.append(len(data.src))\n",
    "  max_len_eng.append(len(data.trg))\n",
    "  if count < 3 :\n",
    "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
    "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
    "    print()\n",
    "  count += 1\n",
    "\n",
    "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\n",
    "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))\n",
    "\n",
    "bert_max_len = tokenizer.model_max_length\n",
    "assert max(max_len_ger) <= bert_max_len, \"버트 최대 입력 길이 초과\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4eh7BABetH1",
    "outputId": "9eda617c-f1bf-4c0d-c7ee-7f798856a35d"
   },
   "outputs": [],
   "source": [
    "# # 첫번째 배치에서 하나의 문장 정보 출력\n",
    "# for i, batch in enumerate(train_iterator):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "\n",
    "#     print(f\"첫 번째 배치 크기: {src.shape}\") # 128개의 문장 중 가장 긴 문장의 길이가 35\n",
    "\n",
    "#     for i in range(0, 2): \n",
    "#         print(src[i])\n",
    "\n",
    "#     # 첫 번째 배치만 확인\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5,    84,    14,   199,    23,   393,   421,  2812,   670,     8,\n",
      "           741,  1173,    42,    20,  6308, 14111,    14, 17305,     6,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    5,   156,    21, 28089,    99,     8,   127,  1401,    14,   488,\n",
      "            28, 21842,    79,    86,    13,  5099,     8,    65,    52,    21,\n",
      "         27151,   312,    33,    13,  3385,  4162,  2068,    19,   529,    56,\n",
      "            16,  1382,    20,  4979,   613,    19,  4979,  3608,  3636,    16,\n",
      "          1382,    20,   529,     6,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 배치에서 하나의 문장 정보 출력\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    print(src[:2])\n",
    "\n",
    "    attention_mask = (src != SRC.pad_token).type(torch.long)\n",
    "    print(attention_mask[:2]) #okenizer.sep_token_id\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "[SEP]:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "[CLS]:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n",
      "[SEP]:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src[:2]\n",
    "\n",
    "    last_hidden_state = bert_model(src).last_hidden_state\n",
    "    \n",
    "    # [CLS] 및 [SEP] 토큰의 인덱스 가져오기\n",
    "    # cls_index = 0\n",
    "    cls_indices = torch.tensor([0])  # [CLS] 토큰의 인덱스\n",
    "    sep_indices = (src == tokenizer.sep_token_id).nonzero()[:, 1]  # [SEP] 토큰의 인덱스\n",
    "    \n",
    "    # 각 배치에 대해 [CLS] 및 [SEP] 토큰의 임베딩을 0으로 설정\n",
    "    for batch_idx in range(last_hidden_state.size(0)):\n",
    "        last_hidden_state[batch_idx, cls_indices, :] = 0  # [CLS] 토큰의 임베딩을 0으로 설정\n",
    "        last_hidden_state[batch_idx, sep_indices[batch_idx], :] = 0  # [SEP] 토큰의 임베딩을 0으로 설정\n",
    "    \n",
    "    for i in range(src.size(0)):\n",
    "        print(\"[CLS]: \", last_hidden_state[i,cls_indices,:10])\n",
    "        print(\"[SEP]: \", last_hidden_state[i,sep_indices[i],:10], '\\n')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-HT1C6kfQG6"
   },
   "source": [
    "#### **Multi Head Attention 아키텍처**\n",
    "\n",
    "* 어텐션(attention)은 <b>세 가지 요소</b>를 입력으로 받습니다.\n",
    "    * <b>쿼리(queries)</b>, <b>키(keys)</b>, <b>값(values)</b>\n",
    "    * 현재 구현에서는 Query, Key, Value의 차원이 모두 같습니다.\n",
    "* 하이퍼 파라미터(hyperparameter)\n",
    "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
    "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
    "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyqLWPd24fqv"
   },
   "source": [
    "**멀티 헤드 어텐션을 구현 :**\n",
    "\n",
    "1. 어텐션 헤드 수 정의\n",
    "\n",
    "1. 프로젝션 수행: 입력 벡터를 여러 헤드로 분할하기 위해 선형 프로젝션을 수행합니다. 이는 입력 벡터를 각 어텐션 헤드의 차원으로 분할하는 과정입니다.\n",
    "\n",
    "1. 각 헤드에 대한 어텐션 계산: 분할된 입력 벡터를 각 어텐션 헤드에 대해 어텐션을 계산합니다. 각 헤드에서는 각자의 쿼리(Q), 키(K), 값(V)에 대한 어텐션을 계산합니다.\n",
    "\n",
    "1. 헤드 결합: 각 헤드에서 계산된 어텐션 값을 결합하여 최종 어텐션 값으로 합칩니다. 이렇게 하면 각 헤드가 다르게 강조한 정보를 결합하여 하나의 표현으로 얻을 수 있습니다.\n",
    "- ~??~ 멀티헤드 쓰려면 임베딩 차원을 쪼개니까 병령 처리로 속도만 빨라지고 아무리 다양한 각도로 본다해도 쪼개면 부분적으로만 보니까 직관적으로 생각해봤을 때는 오히려 성능이 더 떨어지지 않나?\n",
    "    -  다른 시각으로 볼 수 있다는 메리트가 더 큰 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "ohBIfgOJiL0a"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # hidden_dim이 n_heads로 나누어 떨어지지 않으면 AssertionError를 발생시킴\n",
    "        # 이때 은닉 차원을 어텐션 헤드의 수로 나누어 떨어지게 하는 것은 헤드 간에 정보를 공유하고 연산을 병렬화하는 데 도움이 됨\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원   / fc_q의 결과 디멘젼을 n_heads개로 쪼개서 사용\n",
    "\n",
    "        # 쿼리(Q)를 계산하기 위한 레이어 ;입력을 쿼리로 변환하기 위한 레이어\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
    "\n",
    "        # 어텐션 메커니즘에서 사용되는 출력을 계산하기 위한 레이어\n",
    "        # : Q,K,V를 이용하여 어텐션을 계산한 후, 이를 조합하여 최종 출력을 생성\n",
    "          # 이 과정에서 self.fc_o는 어텐션 값을 변환하여 최종 출력을 계산\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query: [batch_size, query_len, hidden_dim]   / query_len : 단어 개수\n",
    "        # key: [batch_size, key_len, hidden_dim]\n",
    "        # value: [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q: [batch_size, query_len, hidden_dim]\n",
    "        # K: [batch_size, key_len, hidden_dim]\n",
    "        # V: [batch_size, value_len, hidden_dim]\n",
    "\n",
    "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
    "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
    "        #   Q,K,V 결과 값을 h개로 나눠 사용 ;n_heads개 각각 마다 head_dim 만큼의 크기로 차원을 가지도록 만들어서 ;h개의 Q,K,V 만듦\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # ?? view(batch_size, self.n_heads, -1, self.head_dim) 이렇게 하면 permute 안해도 되지 않나\n",
    "        \n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        # K: [batch_size, n_heads, key_len, head_dim]\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        # Attention Energy 계산\n",
    "        # 각 head마다 Q,K 서로 곱하고 scale로 나눠서 energy 구함\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # energy: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 마스크(mask)를 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기 -> softmax에 들어간 값이 거의 0%가 나오게\n",
    "            energy = energy.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
    "        attention = torch.softmax(energy, dim=-1) # 마지막 차원을 따라 소프트맥스 연산 수행\n",
    "        # @ 각 행의 합이 1\n",
    "        \n",
    "        # attention: [batch_size, n_heads, query_len, key_len]\n",
    "\n",
    "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
    "        # 위에서 softmax를 취해서 나온 attention 가중치 * V 해서 어텐션 밸값을 결과적으로 만들어줌\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x: [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous() # contiguous : 메모리 레이아웃을 연속적으로 만들어줌 -> 행렬 연산을 효율적으로 수행하기 위해\n",
    "        # ?? 왜 permute -> 다음 코드에서 n_heads, head_dim를 hidden_dim로 연결하기 위해\n",
    "        \n",
    "        # x: [batch_size, query_len, n_heads, head_dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hidden_dim) # ;일자로 쭉 늘어뜨림; concat\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x: [batch_size, query_len, hidden_dim]\n",
    "\n",
    "        return x, attention # 나중에 시각화도 하려고 attention 스코어값도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4-71fGFUQ0P"
   },
   "source": [
    "#### **Position-wise Feedforward 아키텍처**\n",
    "\n",
    "* 입력과 출력의 차원이 동일합니다.\n",
    "* 하이퍼 파라미터(hyperparameter)\n",
    "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
    "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
    "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "yBXPWolrUeYj"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        # hidden_dim -> hidden_dim : 입출력 차원 동일\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x: [batch_size, seq_len, pf_dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "0jn4VCWdXhK5"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # 총 6개 레이어 <- 디코더가 이렇게 구성됨. 이걸 여러번 중첩\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        # trg_mask: [batch_size, trg_len] \n",
    "        # src_mask: [batch_size, src_len]\n",
    "        # mask 크기 잘 못 쓴 듯 : 두 코드 참고\n",
    "            # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "            # energy.masked_fill(mask==0, -1e10)\n",
    "\n",
    "        # 1. self attention\n",
    "        # 자기 자신에 대하여 어텐션(attention)\n",
    "        # 1~3rd : q,k,v는 모두 자기 자신(trg)을 넣어서 만듦\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # 2. dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # 3. encoder attention\n",
    "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
    "        # 인코더 디코더 어텐션 수행 : 인코더에서 정보를 가져옴\n",
    "        # Q - trg : 디코드에 포함되어 있는 출력 단어들에 대한 정보\n",
    "        # K - enc_src : 인코더에서 가장 마지막 출력 값으로 나온 값\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "if flag_pretrained_emb :\n",
    "    HIDDEN_DIM = len(trg_embedding.weight[1])\n",
    "else:\n",
    "    HIDDEN_DIM = 256 #\n",
    "print(HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_HIDDEN_DIM = bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=500): # max_length - change\n",
    "        super().__init__()\n",
    "\n",
    "        # self.device = device\n",
    "        # self.dropout = nn.Dropout(dropout_ratio)\n",
    "        # self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "        # self.bert = bert_model\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        attention_mask = (src != SRC.pad_token).type(torch.long)\n",
    "        with torch.no_grad():\n",
    "            enc_src = bert_model(src, attention_mask=attention_mask)[0][:,:,:HIDDEN_DIM]\n",
    "\n",
    "        # enc_src: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        cls_indices = torch.tensor([0])  # [CLS] 토큰의 인덱스\n",
    "        sep_indices = (src == tokenizer.sep_token_id).nonzero()[:, 1]  # [SEP] 토큰의 인덱스\n",
    "        \n",
    "        # 각 배치에 대해 [CLS] 및 [SEP] 토큰의 임베딩을 0으로 설정\n",
    "        for batch_idx in range(enc_src.size(0)):\n",
    "            enc_src[batch_idx, cls_indices, :] = 0  # [CLS] 토큰의 임베딩을 0으로 설정\n",
    "            enc_src[batch_idx, sep_indices[batch_idx], :] = 0  \n",
    "\n",
    "        return enc_src # 마지막 레이어의 출력을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "X64at7IuWQcm"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=max(max_len_ger)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # output_dim : 단어 개수\n",
    "        if flag_pretrained_emb :\n",
    "            self.tok_embedding = trg_embedding\n",
    "        else :        \n",
    "            self.tok_embedding = nn.Embedding(output_dim, hidden_dim)        # max_length : seq len \n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, BERT_HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(BERT_HIDDEN_DIM, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "\n",
    "        # trg: [batch_size, trg_len] /타겟 문장에 대한 정보\n",
    "        # enc_src: [batch_size, src_len, hidden_dim] /인코더 마지막 출력값\n",
    "        # trg_mask: [batch_size, trg_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos: [batch_size, trg_len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # 소스 마스크와 타겟 마스크 모두 사용\n",
    "            # trg, attention = layer(trg, enc_src, trg_mask, src_mask) \n",
    "            # trg, attention = layer(trg, self.fc2(self.fc1(enc_src)), trg_mask, src_mask) \n",
    "            \n",
    "            trg, attention = layer(trg, self.fc2(self.dropout(torch.relu((self.fc1(enc_src))))), trg_mask, src_mask)\n",
    "\n",
    "\n",
    "        # trg: [batch_size, trg_len, hidden_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        # 출력을 위한 fc 거침\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁il', '▁et', 'udi', 'e', '▁leurs', '▁habitudes']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('il etudie leurs habitudes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b50rQACqW3xX"
   },
   "source": [
    "#### **트랜스포머(Transformer) 아키텍처**\n",
    "\n",
    "* 최종적인 전체 트랜스포머(Transformer) 모델을 정의합니다.\n",
    "* 입력이 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "hBGN8VyvW0Et"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "\n",
    "        # G () : 패딩 토큰과 일치하지 않는 위치를 찾음\n",
    "        #   unsqueeze(2) : 브로드캐스팅(broadcasting) 연산을 수행할 수 있도록\n",
    "        #   src_mask : 패딩 토큰을 포함하지 않는 입력 시퀀스의 위치에는 True\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        # 1. 소스 문장과 동일하게 pad 마스킹  \n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 0 0\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # 2. 별도의 마스크 하나 더 만듦 : 앞쪽 단어만 볼 수 있게\n",
    "        \"\"\" (마스크 예시)\n",
    "        1 0 0 0 0\n",
    "        1 1 0 0 0\n",
    "        1 1 1 0 0\n",
    "        1 1 1 1 0\n",
    "        1 1 1 1 1\n",
    "        \"\"\"\n",
    "        # G .tril()은 행렬의 하삼각 부분을 반환. 모두 1로 채워진 행렬에 적용\n",
    "          # (trg_len, trg_len)은 정사각형 행렬을 생성하기 위해\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "\n",
    "        # G 브로드캐스팅은\n",
    "          # 두 배열의 차원 수가 다르면 차원 수가 더 적은 배열의 형상이 더 많은 배열의 형상에 맞춰지도록 자동으로 확장\n",
    "          # 두 배열의 크기가 어느 한 차원에서 일치하지 않으면 크기가 1인 차원이 다른 배열의 크기에 맞추어 확장\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "\n",
    "        # enc_src = self.encoder(src, src_mask)\n",
    "        enc_src = self.encoder(src)\n",
    "        \n",
    "        # output : 번역 결과\n",
    "        # 디코더는 매번 enc_src(인코더의 출력값)을 어텐션\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 32000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"어휘 사전의 크기:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnBKjEFFqHrV"
   },
   "source": [
    "#### **학습(Training)**\n",
    "\n",
    "* 하이퍼 파라미터 설정 및 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "vJG6xhUaXZ32"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 0 # 아직 사용x\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_LAYERS = 3 # 아직 사용x\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8 # 아직 사용x\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512 # 아직 사용x\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1 # 아직 사용x\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "rVgG8VOYXbIk"
   },
   "outputs": [],
   "source": [
    "# G SRC 텍스트 필드에서 pad_token 속성을 사용하여 패딩 토큰의 문자열을 가져온 다음, stoi (string to index) 속성을 사용하여 해당 문자열을 정수 인덱스로 변환\n",
    "SRC_PAD_IDX = SRC.pad_token\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
    "\n",
    "# Transformer 객체 선언\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.tok_embedding.weight                                           True\n",
      "decoder.pos_embedding.weight                                           True\n",
      "decoder.fc1.weight                                                     True\n",
      "decoder.fc1.bias                                                       True\n",
      "decoder.fc2.weight                                                     True\n",
      "decoder.fc2.bias                                                       True\n",
      "decoder.layers.0.self_attn_layer_norm.weight                           True\n",
      "decoder.layers.0.self_attn_layer_norm.bias                             True\n",
      "decoder.layers.0.enc_attn_layer_norm.weight                            True\n",
      "decoder.layers.0.enc_attn_layer_norm.bias                              True\n",
      "decoder.layers.0.ff_layer_norm.weight                                  True\n",
      "decoder.layers.0.ff_layer_norm.bias                                    True\n",
      "decoder.layers.0.self_attention.fc_q.weight                            True\n",
      "decoder.layers.0.self_attention.fc_q.bias                              True\n",
      "decoder.layers.0.self_attention.fc_k.weight                            True\n",
      "decoder.layers.0.self_attention.fc_k.bias                              True\n",
      "decoder.layers.0.self_attention.fc_v.weight                            True\n",
      "decoder.layers.0.self_attention.fc_v.bias                              True\n",
      "decoder.layers.0.self_attention.fc_o.weight                            True\n",
      "decoder.layers.0.self_attention.fc_o.bias                              True\n",
      "decoder.layers.0.encoder_attention.fc_q.weight                         True\n",
      "decoder.layers.0.encoder_attention.fc_q.bias                           True\n",
      "decoder.layers.0.encoder_attention.fc_k.weight                         True\n",
      "decoder.layers.0.encoder_attention.fc_k.bias                           True\n",
      "decoder.layers.0.encoder_attention.fc_v.weight                         True\n",
      "decoder.layers.0.encoder_attention.fc_v.bias                           True\n",
      "decoder.layers.0.encoder_attention.fc_o.weight                         True\n",
      "decoder.layers.0.encoder_attention.fc_o.bias                           True\n",
      "decoder.layers.0.positionwise_feedforward.fc_1.weight                  True\n",
      "decoder.layers.0.positionwise_feedforward.fc_1.bias                    True\n",
      "decoder.layers.0.positionwise_feedforward.fc_2.weight                  True\n",
      "decoder.layers.0.positionwise_feedforward.fc_2.bias                    True\n",
      "decoder.layers.1.self_attn_layer_norm.weight                           True\n",
      "decoder.layers.1.self_attn_layer_norm.bias                             True\n",
      "decoder.layers.1.enc_attn_layer_norm.weight                            True\n",
      "decoder.layers.1.enc_attn_layer_norm.bias                              True\n",
      "decoder.layers.1.ff_layer_norm.weight                                  True\n",
      "decoder.layers.1.ff_layer_norm.bias                                    True\n",
      "decoder.layers.1.self_attention.fc_q.weight                            True\n",
      "decoder.layers.1.self_attention.fc_q.bias                              True\n",
      "decoder.layers.1.self_attention.fc_k.weight                            True\n",
      "decoder.layers.1.self_attention.fc_k.bias                              True\n",
      "decoder.layers.1.self_attention.fc_v.weight                            True\n",
      "decoder.layers.1.self_attention.fc_v.bias                              True\n",
      "decoder.layers.1.self_attention.fc_o.weight                            True\n",
      "decoder.layers.1.self_attention.fc_o.bias                              True\n",
      "decoder.layers.1.encoder_attention.fc_q.weight                         True\n",
      "decoder.layers.1.encoder_attention.fc_q.bias                           True\n",
      "decoder.layers.1.encoder_attention.fc_k.weight                         True\n",
      "decoder.layers.1.encoder_attention.fc_k.bias                           True\n",
      "decoder.layers.1.encoder_attention.fc_v.weight                         True\n",
      "decoder.layers.1.encoder_attention.fc_v.bias                           True\n",
      "decoder.layers.1.encoder_attention.fc_o.weight                         True\n",
      "decoder.layers.1.encoder_attention.fc_o.bias                           True\n",
      "decoder.layers.1.positionwise_feedforward.fc_1.weight                  True\n",
      "decoder.layers.1.positionwise_feedforward.fc_1.bias                    True\n",
      "decoder.layers.1.positionwise_feedforward.fc_2.weight                  True\n",
      "decoder.layers.1.positionwise_feedforward.fc_2.bias                    True\n",
      "decoder.layers.2.self_attn_layer_norm.weight                           True\n",
      "decoder.layers.2.self_attn_layer_norm.bias                             True\n",
      "decoder.layers.2.enc_attn_layer_norm.weight                            True\n",
      "decoder.layers.2.enc_attn_layer_norm.bias                              True\n",
      "decoder.layers.2.ff_layer_norm.weight                                  True\n",
      "decoder.layers.2.ff_layer_norm.bias                                    True\n",
      "decoder.layers.2.self_attention.fc_q.weight                            True\n",
      "decoder.layers.2.self_attention.fc_q.bias                              True\n",
      "decoder.layers.2.self_attention.fc_k.weight                            True\n",
      "decoder.layers.2.self_attention.fc_k.bias                              True\n",
      "decoder.layers.2.self_attention.fc_v.weight                            True\n",
      "decoder.layers.2.self_attention.fc_v.bias                              True\n",
      "decoder.layers.2.self_attention.fc_o.weight                            True\n",
      "decoder.layers.2.self_attention.fc_o.bias                              True\n",
      "decoder.layers.2.encoder_attention.fc_q.weight                         True\n",
      "decoder.layers.2.encoder_attention.fc_q.bias                           True\n",
      "decoder.layers.2.encoder_attention.fc_k.weight                         True\n",
      "decoder.layers.2.encoder_attention.fc_k.bias                           True\n",
      "decoder.layers.2.encoder_attention.fc_v.weight                         True\n",
      "decoder.layers.2.encoder_attention.fc_v.bias                           True\n",
      "decoder.layers.2.encoder_attention.fc_o.weight                         True\n",
      "decoder.layers.2.encoder_attention.fc_o.bias                           True\n",
      "decoder.layers.2.positionwise_feedforward.fc_1.weight                  True\n",
      "decoder.layers.2.positionwise_feedforward.fc_1.bias                    True\n",
      "decoder.layers.2.positionwise_feedforward.fc_2.weight                  True\n",
      "decoder.layers.2.positionwise_feedforward.fc_2.bias                    True\n",
      "decoder.fc_out.weight                                                  True\n",
      "decoder.fc_out.bias                                                    True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # <는 왼쪽 정렬, 숫자는 해당 필드의 최소 폭\n",
    "    print(\"{:<70} {}\".format(name, param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B1BkZkUqQjo"
   },
   "source": [
    "* **모델 가중치 파라미터 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnCu5WAyXmes",
    "outputId": "8d62d4bb-628f-40b3-aa44-f2b299335b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17,817,558 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    # G numel() : 해당 텐서의 총 요소 수 반환\n",
    "    #   다차원 텐서의 경우 모든 차원의 크기를 곱한 값 반환\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEY3bppUXndU",
    "outputId": "60332042-53d5-4fb1-e397-ec4edcae7dbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder()\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(29142, 256)\n",
       "    (pos_embedding): Embedding(395, 256)\n",
       "    (fc1): Linear(in_features=256, out_features=768, bias=True)\n",
       "    (fc2): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=29142, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m): # m(모듈): 각 레이어\n",
    "    # G 차원이 1보다 작으면 해당 텐서가 벡터이며, 일반적으로 이러한 경우에는 Xavier 초기화를 적용x\n",
    "      # 가중치 텐서의 차원이 1보다 작은 경우는 보통 편향(bias)을 나타냅니다. 대부분의 PyTorch 레이어는 가중치와 편향을 함께 가지고 있습니다\n",
    "      # Xavier 초기화는 주로 가중치에 대해서만 적용되기 때문에 가중치 텐서의 차원이 1보다 작은 경우 Xavier 초기화를 적용하지 않는 것이 일반적\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        # G Xavier 초기화는 각 가중치를 평균이 0이고 분산이 2/(입력 개수 + 출력 개수)인 분포에서 랜덤하게 샘플링하여 초기화\n",
    "          # 분산을 2/(입력 개수 + 출력 개수)로 설정하는 것은 효율적인 초기화 방법으로, 입력과 출력의 개수가 많은 경우에도 가중치가 적절한 크기로 초기화\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "# G apply : 모델 내의 각 매개변수에 함수를 적용하는 데 사용됩\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsdTndLDqWQf"
   },
   "source": [
    "* 학습 및 평가 함수 정의\n",
    "    * 기본적인 Seq2Seq 모델과 거의 유사하게 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "m6-92JSlXrwM"
   },
   "outputs": [],
   "source": [
    "# Adam optimizer로 학습 최적화\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "tcIltwlRXssU"
   },
   "outputs": [],
   "source": [
    "# 모델 학습(train) 함수\n",
    "from tqdm import tqdm\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train() # 학습 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 전체 학습 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 출력 단어의 마지막 인덱스(<eos>)는 제외 ?? -> eos는 디코더의 입력이 아니라 디코더의 출력\n",
    "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "        # G 모델은 입력 시퀀스와 이전 부분의 목표 시퀀스를 사용하여 예측을 수행\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "        # output: [배치 크기, trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기, trg_len]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "        # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() # 기울기(gradient) 계산\n",
    "\n",
    "        # 기울기(gradient) clipping 진행\n",
    "        # G 그래디언트의 전체 노름(norm)을 계산합니다. 그런 다음, 노름을 지정된 임계값으로 클리핑, 그래디언트의 크기가 제한\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "69a5vBggXt4M"
   },
   "outputs": [],
   "source": [
    "# 모델 평가(evaluate) 함수\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 전체 평가 데이터를 확인하며\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
    "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            # output: [배치 크기, trg_len - 1, output_dim]\n",
    "            # trg: [배치 크기, trg_len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
    "            # trg: [배치 크기 * trg len - 1]\n",
    "\n",
    "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # 전체 손실 값 계산\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdmhWLNcs76v"
   },
   "source": [
    "* 학습(training) 및 검증(validation) 진행\n",
    "    * **학습 횟수(epoch)**: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "HPjtaQ6CXvGk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603.27734375\n",
      "31266.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:395: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 메모리에 현재 할당된 메모리량을 바이트 단위로 반환\n",
    "print(torch.cuda.memory_allocated()/ 1024**2)\n",
    "print(torch.cuda.memory_cached()/ 1024**2)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "plUPXH4UYKEU"
   },
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "# 1. 하나의 sentence가 들어왔을 때\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, logging=True): # max_len - change\n",
    "    model.eval() # 평가 모드\n",
    "    \n",
    "    '''\n",
    "    # 토큰화\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else: # G  문자열이 아닌 경우 이미 토큰화된 문장을 가정하고 각 토큰을 소문자로 변환\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    '''\n",
    "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
    "    src_indexes = [src_field.init_token] + sentence + [src_field.eos_token]\n",
    "    # if logging:\n",
    "    #     print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    # # 모델의 입력으로 넣기 위해 각 단어를 인덱스로 바꿈\n",
    "    # src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    if logging:\n",
    "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    # 소스 문장에 따른 마스크 생성\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    # attention_mask = (src_tensor != src_field.pad_token).type(torch.long)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 출력 값 구하기\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor)\n",
    "    \n",
    "    # cls_indices = torch.tensor([0])  # [CLS] 토큰의 인덱스\n",
    "    # sep_indices = (src == tokenizer.sep_token_id).nonzero()[:, 1]  # [SEP] 토큰의 인덱스\n",
    "    \n",
    "    # # 각 배치에 대해 [CLS] 및 [SEP] 토큰의 임베딩을 0으로 설정\n",
    "    # for batch_idx in range(enc_src.size(0)):\n",
    "    #     enc_src[batch_idx, 0, :] = 0  # [CLS] 토큰의 임베딩을 0으로 설정\n",
    "    #     enc_src[batch_idx, -1, :] = 0  \n",
    "    # print(enc_src)\n",
    "\n",
    "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
    "    # ; 실제 출력 문장은 <sos> 토큰부터 출발해서\n",
    "      # max_len까지 하나씩 반복적으로 모델의 디코더에 넣어서 출력 만듦\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        # ?? for문 바깥 위쪽으로 빼면 안되나 -> trg_indexes와 별개로 해야함. 코드 쭉 읽어보기\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device) \n",
    "\n",
    "        # 출력 문장에 따른 마스크 생성\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # 매번 디코더에 넣었을 때 마지막 단어가 출력 문장으로써 하나씩 추가됨\n",
    "        # 출력 문장에서 가장 마지막 단어만 사용\n",
    "        # G output에서 가장 높은 확률을 가지는 토큰의 인덱스를 추출\n",
    "          # 2 : 텐서의 차원(axis) 2를 따라 가장 큰 값을 갖는 인덱스를 반환\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "\n",
    "        # <eos>를 만나는 순간 끝 -> 이때까지 출력된 모든 단어들이 전체 출력 문장이 됨\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어(문자열)로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "C7hsjkOKb3HS"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def show_bleu(data, src_field, trg_field, model, device, max_len=50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    index = 0\n",
    "\n",
    "    for datum in data:\n",
    "        # G 데이터를 사전 형태로 변환한 후, 'src' 키를 사용하여 해당 데이터의 소스 문장을 추출\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "\n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, logging=False)\n",
    "\n",
    "        # 마지막 <eos> 토큰 제거\n",
    "        pred_trg = pred_trg[:-1]\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "\n",
    "        index += 1\n",
    "        if (index + 1) % 500 == 0:\n",
    "            print(f\"[{index + 1}/{len(data)}]\")\n",
    "            print(f\"예측: {pred_trg}\")\n",
    "            print(f\"정답: {trg}\")\n",
    "\n",
    "    # G 아래는 디폴트값과 일치 \n",
    "      # max_n : 사용할 최대 n-gram 크기를 지정\n",
    "      # eights : 각 n-gram에 대한 가중치를 지정\n",
    "    bleu = bleu_score(pred_trgs, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "    print(f'Total BLEU Score = {bleu*100:.2f}')\n",
    "\n",
    "    individual_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
    "    individual_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
    "    individual_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
    "    individual_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
    "\n",
    "    print(f'Individual BLEU1 score = {individual_bleu1_score*100:.2f}')\n",
    "    print(f'Individual BLEU2 score = {individual_bleu2_score*100:.2f}')\n",
    "    print(f'Individual BLEU3 score = {individual_bleu3_score*100:.2f}')\n",
    "    print(f'Individual BLEU4 score = {individual_bleu4_score*100:.2f}')\n",
    "\n",
    "    # G 각 n-gram에 대한 가중치를 모두 동일하게 설정하는 대신에 ㅁ-gram까지의 n-gram을 고려\n",
    "      # cf) BLEU: 더 긴 n-gram에 대해서는 더 높은 가중치를 부여\n",
    "    cumulative_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
    "    cumulative_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/2, 1/2, 0, 0])\n",
    "    cumulative_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/3, 1/3, 1/3, 0])\n",
    "    cumulative_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "    print(f'Cumulative BLEU1 score = {cumulative_bleu1_score*100:.2f}')\n",
    "    print(f'Cumulative BLEU2 score = {cumulative_bleu2_score*100:.2f}')\n",
    "    print(f'Cumulative BLEU3 score = {cumulative_bleu3_score*100:.2f}')\n",
    "    print(f'Cumulative BLEU4 score = {cumulative_bleu4_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTe-5FdvXwdE",
    "outputId": "4da09fee-2804-40d2-9d8b-d668ed4056af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 | Time: 4m 24s | Train PPL: 178.501 | Val PPL: 39.377\n",
      "02 | Time: 4m 22s | Train PPL: 49.951 | Val PPL: 15.970\n",
      "03 | Time: 4m 25s | Train PPL: 22.504 | Val PPL: 9.812\n",
      "04 | Time: 4m 22s | Train PPL: 13.399 | Val PPL: 7.580\n",
      "[500/3119]\n",
      "예측: ['besides', 'how', 'did', 'this', 'submarine', 'escape', 'to', 'any', 'public', 'notice']\n",
      "정답: ['besides', 'how', 'could', 'the', 'assembly', 'of', 'this', 'underwater', 'boat', 'have', 'escaped', 'public', 'notice']\n",
      "[1000/3119]\n",
      "예측: ['then', 'you', 'must', 'have', 'a', 'man', 'of', 'business', 'a', 'friend', 'of', 'business', 'you', 'pay', 'a', '<unk>', 'at', 'a', 'rate', 'rate']\n",
      "정답: ['then', 'you', 'must', 'have', 'some', 'confidant', 'some', 'safe', 'man', 'of', 'business', 'who', 'pays', 'you', 'interest', 'at', 'a', 'fair', 'rate']\n",
      "[1500/3119]\n",
      "예측: ['and', 'did', 'you', 'not', 'discover', 'that', 'your', 'clothes', 'were', 'not', 'wet']\n",
      "정답: ['and', 'was', 'it', 'not', 'discovered', 'that', 'your', 'sheets', 'were', 'unhemmed']\n",
      "[2000/3119]\n",
      "예측: ['ah']\n",
      "정답: ['ah']\n",
      "[2500/3119]\n",
      "예측: ['alas']\n",
      "정답: ['alas']\n",
      "[3000/3119]\n",
      "예측: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'exchange', 'of', 'good', 'and', 'instead', 'of', 'a', 'wound', 'i', 'would', 'grant', 'him', 'all', 'advantages', 'with', 'gratitude', 'for', 'his', 'acceptance']\n",
      "정답: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'every', 'benefit', 'upon', 'him', 'with', 'tears', 'of', 'gratitude', 'at', 'his', 'acceptance']\n",
      "Total BLEU Score = 27.38\n",
      "Individual BLEU1 score = 57.18\n",
      "Individual BLEU2 score = 34.27\n",
      "Individual BLEU3 score = 21.23\n",
      "Individual BLEU4 score = 13.51\n",
      "Cumulative BLEU1 score = 57.18\n",
      "Cumulative BLEU2 score = 44.27\n",
      "Cumulative BLEU3 score = 34.65\n",
      "Cumulative BLEU4 score = 27.38\n",
      "05 | Time: 4m 22s | Train PPL: 9.415 | Val PPL: 6.605\n",
      "06 | Time: 4m 22s | Train PPL: 7.314 | Val PPL: 6.019\n",
      "07 | Time: 4m 24s | Train PPL: 6.062 | Val PPL: 5.815\n",
      "08 | Time: 4m 22s | Train PPL: 5.252 | Val PPL: 5.711\n",
      "09 | Time: 4m 24s | Train PPL: 4.674 | Val PPL: 5.579\n",
      "[500/3119]\n",
      "예측: ['besides', 'how', 'did', 'this', 'submarine', '<unk>', 'lead', 'to', 'any', 'public', 'notice']\n",
      "정답: ['besides', 'how', 'could', 'the', 'assembly', 'of', 'this', 'underwater', 'boat', 'have', 'escaped', 'public', 'notice']\n",
      "[1000/3119]\n",
      "예측: ['then', 'you', 'must', 'have', 'a', 'friend', 'of', 'business', 'a', 'friend', 'of', 'business', 'on', 'whom', 'you', 'pay', 'a', 'literary', 'rate']\n",
      "정답: ['then', 'you', 'must', 'have', 'some', 'confidant', 'some', 'safe', 'man', 'of', 'business', 'who', 'pays', 'you', 'interest', 'at', 'a', 'fair', 'rate']\n",
      "[1500/3119]\n",
      "예측: ['and', 'did', 'you', 'not', 'discover', 'that', 'your', 'linen', 'were', 'not', 'soaked']\n",
      "정답: ['and', 'was', 'it', 'not', 'discovered', 'that', 'your', 'sheets', 'were', 'unhemmed']\n",
      "[2000/3119]\n",
      "예측: ['ah']\n",
      "정답: ['ah']\n",
      "[2500/3119]\n",
      "예측: ['alas']\n",
      "정답: ['alas']\n",
      "[3000/3119]\n",
      "예측: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'a', 'wound', 'i', 'would', 'bestow', 'all', 'the', 'advantages', 'with', 'gratitude', 'for', 'her', 'acceptance']\n",
      "정답: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'every', 'benefit', 'upon', 'him', 'with', 'tears', 'of', 'gratitude', 'at', 'his', 'acceptance']\n",
      "Total BLEU Score = 32.40\n",
      "Individual BLEU1 score = 62.18\n",
      "Individual BLEU2 score = 39.42\n",
      "Individual BLEU3 score = 25.88\n",
      "Individual BLEU4 score = 17.37\n",
      "Cumulative BLEU1 score = 62.18\n",
      "Cumulative BLEU2 score = 49.51\n",
      "Cumulative BLEU3 score = 39.88\n",
      "Cumulative BLEU4 score = 32.40\n",
      "10 | Time: 4m 22s | Train PPL: 4.257 | Val PPL: 5.560\n",
      "11 | Time: 4m 22s | Train PPL: 3.938 | Val PPL: 5.582\n",
      "12 | Time: 4m 24s | Train PPL: 3.683 | Val PPL: 5.582\n",
      "13 | Time: 4m 24s | Train PPL: 3.476 | Val PPL: 5.623\n",
      "[500/3119]\n",
      "예측: ['furthermore', 'how', 'did', 'the', 'contrivance', 'of', 'this', 'submarine', 'escaped', 'from', 'any', 'public', 'notice']\n",
      "정답: ['besides', 'how', 'could', 'the', 'assembly', 'of', 'this', 'underwater', 'boat', 'have', 'escaped', 'public', 'notice']\n",
      "[1000/3119]\n",
      "예측: ['then', 'you', 'must', 'have', 'a', 'friend', 'of', 'business', 'a', 'friend', 'of', 'business', 'on', 'whom', 'you', 'pay', 'money', 'at', 'a', 'equable', 'rate']\n",
      "정답: ['then', 'you', 'must', 'have', 'some', 'confidant', 'some', 'safe', 'man', 'of', 'business', 'who', 'pays', 'you', 'interest', 'at', 'a', 'fair', 'rate']\n",
      "[1500/3119]\n",
      "예측: ['and', 'did', 'you', 'not', 'discover', 'that', 'your', 'linen', 'were', 'not', 'wet']\n",
      "정답: ['and', 'was', 'it', 'not', 'discovered', 'that', 'your', 'sheets', 'were', 'unhemmed']\n",
      "[2000/3119]\n",
      "예측: ['ah']\n",
      "정답: ['ah']\n",
      "[2500/3119]\n",
      "예측: ['alas']\n",
      "정답: ['alas']\n",
      "[3000/3119]\n",
      "예측: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'all', 'the', 'advantages', 'for', 'it', 'with', 'tears', 'of', 'gratitude', 'for', 'its', 'acceptance']\n",
      "정답: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'every', 'benefit', 'upon', 'him', 'with', 'tears', 'of', 'gratitude', 'at', 'his', 'acceptance']\n",
      "Total BLEU Score = 33.08\n",
      "Individual BLEU1 score = 62.20\n",
      "Individual BLEU2 score = 39.77\n",
      "Individual BLEU3 score = 26.54\n",
      "Individual BLEU4 score = 18.23\n",
      "Cumulative BLEU1 score = 62.20\n",
      "Cumulative BLEU2 score = 49.73\n",
      "Cumulative BLEU3 score = 40.34\n",
      "Cumulative BLEU4 score = 33.08\n",
      "15 | Time: 4m 23s | Train PPL: 3.157 | Val PPL: 5.694\n",
      "16 | Time: 4m 23s | Train PPL: 3.032 | Val PPL: 5.814\n",
      "17 | Time: 4m 24s | Train PPL: 2.930 | Val PPL: 5.849\n",
      "18 | Time: 4m 24s | Train PPL: 2.832 | Val PPL: 5.925\n",
      "19 | Time: 4m 22s | Train PPL: 2.755 | Val PPL: 6.029\n",
      "[500/3119]\n",
      "예측: ['furthermore', 'the', 'assembly', 'of', 'this', 'submarine', 'still', 'how', 'has', 'escaped', 'from', 'any', 'public', 'notice']\n",
      "정답: ['besides', 'how', 'could', 'the', 'assembly', 'of', 'this', 'underwater', 'boat', 'have', 'escaped', 'public', 'notice']\n",
      "[1000/3119]\n",
      "예측: ['then', 'you', 'must', 'have', 'a', 'milliners', 'gentleman', 'a', 'friend', 'of', 'business', 'who', 'pays', 'you', 'a', 'fair', 'rate']\n",
      "정답: ['then', 'you', 'must', 'have', 'some', 'confidant', 'some', 'safe', 'man', 'of', 'business', 'who', 'pays', 'you', 'interest', 'at', 'a', 'fair', 'rate']\n",
      "[1500/3119]\n",
      "예측: ['and', 'did', 'not', 'your', 'linen', 'disappeared', 'that', 'you', 'were', 'not', 'stuffed']\n",
      "정답: ['and', 'was', 'it', 'not', 'discovered', 'that', 'your', 'sheets', 'were', 'unhemmed']\n",
      "[2000/3119]\n",
      "예측: ['ah']\n",
      "정답: ['ah']\n",
      "[2500/3119]\n",
      "예측: ['alas']\n",
      "정답: ['alas']\n",
      "[3000/3119]\n",
      "예측: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'all', 'the', 'advantages', 'for', 'his', 'acceptance']\n",
      "정답: ['let', 'him', 'live', 'with', 'me', 'in', 'the', 'interchange', 'of', 'kindness', 'and', 'instead', 'of', 'injury', 'i', 'would', 'bestow', 'every', 'benefit', 'upon', 'him', 'with', 'tears', 'of', 'gratitude', 'at', 'his', 'acceptance']\n",
      "Total BLEU Score = 33.13\n",
      "Individual BLEU1 score = 61.73\n",
      "Individual BLEU2 score = 39.70\n",
      "Individual BLEU3 score = 26.63\n",
      "Individual BLEU4 score = 18.47\n",
      "Cumulative BLEU1 score = 61.73\n",
      "Cumulative BLEU2 score = 49.50\n",
      "Cumulative BLEU3 score = 40.26\n",
      "Cumulative BLEU4 score = 33.13\n",
      "20 | Time: 4m 24s | Train PPL: 2.679 | Val PPL: 6.100\n",
      "21 | Time: 4m 23s | Train PPL: 2.616 | Val PPL: 6.168\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "N_EPOCHS = 40\n",
    "CLIP = 1\n",
    "# 최솟값을 찾기 위한 초기값으로 설정\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    # valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    valid_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # valid_loss가 더 감소한 경우에만 모델 파라미터를 새로운 파일로 기록\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\n",
    "\n",
    "    if epoch % 5 == 4 and epoch != 0:\n",
    "        show_bleu(test_data, SRC, TRG, model, device)\n",
    "        \n",
    "    print(f'{epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s | Train PPL: {math.exp(train_loss):.3f} | Val PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'transformer_last_trainpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "W1oT2_2yYCBM",
    "outputId": "c931d229-a109-44e0-927a-db333bd66b0e"
   },
   "outputs": [],
   "source": [
    "# 학습된 모델 저장\n",
    "# from google.colab import files\n",
    "\n",
    "# G 사용자의 로컬 컴퓨터로 다운로드\n",
    "# files.download('transformer_german_to_english.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvW5ZDUwwJaI"
   },
   "source": [
    "#### **모델 최종 테스트(testing) 결과 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlO_lLD3wJx0",
    "outputId": "1caf37b7-bae6-4ac8-d7a0-e731346546d4"
   },
   "outputs": [],
   "source": [
    "show_bleu(test_data[:2000], SRC, TRG, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_bleu(train_data[:3000], SRC, TRG, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sScSSNYbwKPR",
    "outputId": "ea5d0d5c-5621-4bbc-a3fd-c3f020600625"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('transformer_german_to_english.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIE5KXd5wVUf"
   },
   "source": [
    "#### **나만의 데이터로 모델 사용해보기**\n",
    "\n",
    "* 테스트 데이터셋을 이용해 모델 테스트 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZZufujhxNuO",
    "outputId": "f44942b9-b8e4-4720-ace1-facf12fca3b8"
   },
   "outputs": [],
   "source": [
    "example_idx = 10\n",
    "\n",
    "# G vars: 객체의 속성을 사전 형태로 반환하는 파이썬 내장 함수\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(f'타겟 문장: {trg}')\n",
    "\n",
    "# attention : 8개 헤드로 구성된 어센션 스코어들의 집합\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
    "\n",
    "print(\"모델 출력 결과:\", \" \".join(translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnRoAAEjyckb"
   },
   "source": [
    "* 어텐션 맵(Attention Map) 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lNAb_YKYLmU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker # G 눈금\n",
    "\n",
    "# 각 헤드에 대한 어텐션 스코어 값 출력\n",
    "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "\n",
    "    assert n_rows * n_cols == n_heads\n",
    "\n",
    "    # 출력할 그림 크기 조절\n",
    "    fig = plt.figure(figsize=(15, 25))\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "\n",
    "        # 어텐션(Attention) 스코어 확률 값을 이용해 그리기\n",
    "        # G 배치 차원을 제거\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        # G _attention 배열을 매트릭스 형태로 표시\n",
    "        # G cmap='bone'은 컬러맵을 지정하는 매개변수로, 'bone'은 흑백으로 표시\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        # 눈금 레이블의 크기를 12로 설정\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], rotation=45)\n",
    "        ax.set_yticklabels([''] + translation)\n",
    "\n",
    "        #  x축의 눈금 간격을 설정\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnCvQ5d8YMrj",
    "outputId": "127de942-53f5-4a68-fca1-855d2e2e30a7"
   },
   "outputs": [],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(f'타겟 문장: {trg}')\n",
    "\n",
    "translation, attention = translate_sentence(src, SRC, TRG, model, device, logging=True)\n",
    "\n",
    "print(\"모델 출력 결과:\", \" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "glUG8EnJYPGc",
    "outputId": "e8b923c2-4ea5-4dc1-f58f-0fae8845e956"
   },
   "outputs": [],
   "source": [
    "# display_attention(src, translation, attention)\n",
    "# mother를 출력하기 위해 mutter를 참고했다는 것을 시각적으로 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t07uJBHB0Voo"
   },
   "source": [
    "#### <b>BLEU Score 계산</b>\n",
    "\n",
    "* 학습된 트랜스포머(Transformer) 모델의 BLEU 스코어 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXM8yzc8ZtgE",
    "outputId": "bf36a7a8-e1b1-41cc-e297-9808b466ce5a"
   },
   "outputs": [],
   "source": [
    "show_bleu(test_data, SRC, TRG, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('transformer_last_trainpt'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
