{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "237791fd-a0d2-4955-8c3b-ad16e8169a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# file_name = \"open.zip\"\n",
    "# # output_dir = \"dataset/dataset\"\n",
    "# output_dir = \".\"\n",
    "\n",
    "# format = \"zip\"\n",
    "# shutil.unpack_archive(file_name, output_dir, format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f06ae11e-6d3c-41a9-a45f-01bb55c53d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import random\n",
    "\n",
    "SEED = 777\n",
    "\n",
    "# random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a65005d4-29a2-456b-9472-1f8cd8d4e7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        img_id                 img_path  airplane  airport  bare soil  \\\n",
      "0  TRAIN_00000  ./train/TRAIN_00000.jpg         0        0          1   \n",
      "1  TRAIN_00001  ./train/TRAIN_00001.jpg         0        0          1   \n",
      "2  TRAIN_00002  ./train/TRAIN_00002.jpg         0        0          1   \n",
      "\n",
      "   baseball diamond  basketball court  beach  bridge  buildings  ...  tanks  \\\n",
      "0                 0                 0      0       0          0  ...      0   \n",
      "1                 0                 0      0       0          1  ...      0   \n",
      "2                 0                 0      0       0          0  ...      0   \n",
      "\n",
      "   tennis court  terrace  track  trail  transmission tower  trees  water  \\\n",
      "0             0        0      0      0                   0      1      0   \n",
      "1             0        0      0      0                   0      0      0   \n",
      "2             0        0      0      0                   0      1      0   \n",
      "\n",
      "   wetland  wind turbine  \n",
      "0        0             0  \n",
      "1        0             0  \n",
      "2        0             0  \n",
      "\n",
      "[3 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "96c53775-2de0-4e35-9c23-417ec97567c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       img_id               img_path\n",
      "0  TEST_00000  ./test/TEST_00000.jpg\n",
      "1  TEST_00001  ./test/TEST_00001.jpg\n",
      "2  TEST_00002  ./test/TEST_00002.jpg\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "print(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "86d1e71c-25a8-4dfc-895b-a23bd953e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65496 43665\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train), len(df_test)) # 65496, 43665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3eda90b6-db49-4cdc-93ef-b5b5c126091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "835db7dc-1992-4007-81cb-eba0bec24a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = df_train.hist(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "da8ceef1-f0ef-40d7-a337-5176fa7bebcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms # , models\n",
    "\n",
    "# 이미지 경로와 레이블을 나눕니다\n",
    "image_paths = df_train['img_path'].values\n",
    "labels = df_train.iloc[:, 2:].values  # 레이블 열 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "167af985-1219-4373-a638-9984dda2b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./train/TRAIN_33119.jpg' './train/TRAIN_10054.jpg'\n",
      " './train/TRAIN_53906.jpg' ... './train/TRAIN_00860.jpg'\n",
      " './train/TRAIN_15795.jpg' './train/TRAIN_56422.jpg']\n",
      "['./train/TRAIN_31596.jpg' './train/TRAIN_18550.jpg'\n",
      " './train/TRAIN_36293.jpg' ... './train/TRAIN_39893.jpg'\n",
      " './train/TRAIN_26601.jpg' './train/TRAIN_33248.jpg']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "[[0 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train)\n",
    "print(X_val)\n",
    "print(y_train)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d892fb05-526a-4d27-86b8-bd3cb62830c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 데이터셋 클래스\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx].astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "089e1242-6020-47a1-8897-15d372e98a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(..., labels=None, mode='train'):\n",
    "\n",
    "# def __getitem__(self, idx):\n",
    "#     if self.mode == 'train':\n",
    "#       x = self.data[idx]\n",
    "#       y = self.labels[idx]\n",
    "#       return x, y\n",
    "#     else:\n",
    "#       x = self.data[idx]\n",
    "#       return x\n",
    "\n",
    "# Dataset(..., mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9106bebe-a3ce-431d-a6d2-33f96c01b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 데이터 증강\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Larger size for more complex models\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d775482f-5371-4294-9a11-9971fe23c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SatelliteDataset(X_train, y_train, transform=train_transform)\n",
    "val_dataset = SatelliteDataset(X_val, y_val, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "14c0d45c-2982-42ff-93ba-da0da68be5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "# ResNet50 모델을 불러오고, 최상위 레이어를 제거합니다\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# 분류 레이어를 변경합니다\n",
    "num_classes = labels.shape[1]\n",
    "print(num_classes)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "10667835-b547-44e7-8460-6ef0ccea015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiLabelResNet(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(MultiLabelResNet, self).__init__()\n",
    "#         self.model = models.resnet50(pretrained=True)\n",
    "#         self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.sigmoid(self.model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "165ce574-b3eb-4bf5-9052-b6db0bccda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사전 학습된 레이어들은 학습되지 않도록 동결합니다\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f7d09272-c346-408a-898f-8be9e35db215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f5f47caa-b7af-4ef8-b44f-a96c4b1b68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            # print(images.shape) # [32, 3, 224, 224]\n",
    "            # print(labels.shape) # [32, 60]\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 배치의 평균 손실 값에 배치 크기를 곱하여 현재 배치의 총 손실 값을 계산\n",
    "            #   배치 크기가 달라질 경우를 대비하여 전체 데이터셋에 대한 정확한 평균 손실 값을 계산하기 위함\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        if epoch >= 9:\n",
    "            torch.save(model.state_dict(), f'epoch{epoch}.pt')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "85493679-7f3c-460a-a3f0-53f9f7375f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.1633, Val Loss: 0.1063\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[257], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 배치의 평균 손실 값에 배치 크기를 곱하여 현재 배치의 총 손실 값을 계산\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#   배치 크기가 달라질 경우를 대비하여 전체 데이터셋에 대한 정확한 평균 손실 값을 계산하기 위함\u001b[39;00m\n\u001b[1;32m     19\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:270\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    267\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    273\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50) # scheduler , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "89718cc2-13cf-4e05-bd0a-74b96524ab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('epoch38.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8ea4bb15-0bb4-4a95-bc8a-7495e981b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = outputs > 0.5\n",
    "            # print(preds.shape) # [32, 60]\n",
    "            # corrects: 올바르게 예측된 레이블의 총 수\n",
    "            # print(preds == labels)\n",
    "            # ? 데이콘이 각 데이터의 모든 레이블이 맞아야 맞다고 체점하는지\n",
    "            corrects += (preds == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    # 전체 검증 데이터셋의 모든 레이블에 대한 정확도\n",
    "    accuracy = corrects / (len(val_loader.dataset) * labels.size(1))\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9e3a3f63-a145-4207-b544-096c87f70414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0520, Validation Accuracy: 0.9791\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "bfa193d9-569a-4855-a3fe-127177692d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c3283249-dc90-46e3-8ef0-b3d357460059",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(df_test['img_path'].values, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "76c961a6-b0d5-4b81-a747-b3cde90ddbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, img_paths in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        # outputs = outputs > 0.5\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        \n",
    "        for img_path, output in zip(img_paths, outputs):\n",
    "            img_id = img_path.split('/')[-1].split('.')[0]  # 파일 이름에서 img_id 추출\n",
    "            # predictions.append([img_id] + output.tolist())\n",
    "            predictions.append([img_id] + [col for col in output.tolist()])\n",
    "            # print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ba51cf1b-3f7e-4761-afd8-c77b10847437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43665, 61)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d373304a-6fc3-4669-ba62-b4d02e681cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 새로운 이미지에 대한 예측 함수\n",
    "# def predict_image(model, image_path):\n",
    "#     model.eval()\n",
    "#     image = Image.open(image_path).convert('RGB')\n",
    "#     # transform = transforms.Compose([\n",
    "#     #     transforms.Resize((224, 224)),\n",
    "#     #     transforms.ToTensor()\n",
    "#     # ])\n",
    "#     # image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "#     image = val_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         output = model(image)\n",
    "#     # ?? 제출 양식 왜 확률이지\n",
    "    \n",
    "#     return output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "49afda00-e5c3-4d9c-aced-837ea9330013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# for img_id, img_path in zip(df_test['img_id'], df_test['img_path']):\n",
    "#     pred = predict_image(model, img_path)\n",
    "#     predictions.append([img_id] + pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d0547e1b-2656-4a35-b98a-dfc6ccf681ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       img_id  airplane  airport  bare soil  baseball diamond  \\\n",
      "0  TEST_00000         0        0          0                 0   \n",
      "1  TEST_00001         0        0          0                 0   \n",
      "2  TEST_00002         0        0          0                 0   \n",
      "\n",
      "   basketball court  beach  bridge  buildings  cars  ...  tanks  tennis court  \\\n",
      "0                 0      0       0          0     0  ...      0             0   \n",
      "1                 0      0       0          0     0  ...      0             0   \n",
      "2                 0      0       0          0     0  ...      0             0   \n",
      "\n",
      "   terrace  track  trail  transmission tower  trees  water  wetland  \\\n",
      "0        0      0      0                   0      0      0        0   \n",
      "1        0      0      0                   0      0      0        0   \n",
      "2        0      0      0                   0      0      0        0   \n",
      "\n",
      "   wind turbine  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "\n",
      "[3 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "print(sample_submission.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a562c0b5-426c-4830-bed4-a31fb849679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in df_train.columns if col != 'img_path']\n",
    "df_predictions = pd.DataFrame(predictions, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "3fa9b0b2-62b3-4143-8393-67dc4d30ab58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>airplane</th>\n",
       "      <th>airport</th>\n",
       "      <th>bare soil</th>\n",
       "      <th>baseball diamond</th>\n",
       "      <th>basketball court</th>\n",
       "      <th>beach</th>\n",
       "      <th>bridge</th>\n",
       "      <th>buildings</th>\n",
       "      <th>cars</th>\n",
       "      <th>...</th>\n",
       "      <th>tanks</th>\n",
       "      <th>tennis court</th>\n",
       "      <th>terrace</th>\n",
       "      <th>track</th>\n",
       "      <th>trail</th>\n",
       "      <th>transmission tower</th>\n",
       "      <th>trees</th>\n",
       "      <th>water</th>\n",
       "      <th>wetland</th>\n",
       "      <th>wind turbine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>7.122533e-17</td>\n",
       "      <td>1.252369e-15</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.377356e-26</td>\n",
       "      <td>3.027507e-13</td>\n",
       "      <td>5.802833e-12</td>\n",
       "      <td>2.288399e-18</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>9.502489e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>4.368716e-13</td>\n",
       "      <td>1.307474e-11</td>\n",
       "      <td>5.247762e-09</td>\n",
       "      <td>1.958257e-14</td>\n",
       "      <td>3.864470e-07</td>\n",
       "      <td>1.603567e-08</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.547775e-09</td>\n",
       "      <td>8.717879e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>2.042777e-10</td>\n",
       "      <td>1.642777e-10</td>\n",
       "      <td>0.861272</td>\n",
       "      <td>3.610178e-21</td>\n",
       "      <td>1.523815e-11</td>\n",
       "      <td>3.414202e-14</td>\n",
       "      <td>5.685124e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.998770e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.435036e-11</td>\n",
       "      <td>3.603454e-11</td>\n",
       "      <td>1.638018e-10</td>\n",
       "      <td>1.539852e-11</td>\n",
       "      <td>1.611498e-09</td>\n",
       "      <td>1.255012e-10</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>4.689848e-12</td>\n",
       "      <td>1.003897e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>4.444573e-07</td>\n",
       "      <td>6.065267e-10</td>\n",
       "      <td>0.678852</td>\n",
       "      <td>3.712249e-15</td>\n",
       "      <td>1.368452e-09</td>\n",
       "      <td>2.161503e-10</td>\n",
       "      <td>6.784780e-07</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>9.284833e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.811736e-08</td>\n",
       "      <td>6.522838e-07</td>\n",
       "      <td>2.212666e-07</td>\n",
       "      <td>1.565127e-08</td>\n",
       "      <td>2.063420e-05</td>\n",
       "      <td>6.402930e-07</td>\n",
       "      <td>0.301665</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.140984e-08</td>\n",
       "      <td>1.851786e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>2.090319e-05</td>\n",
       "      <td>5.493608e-05</td>\n",
       "      <td>0.627948</td>\n",
       "      <td>2.127815e-09</td>\n",
       "      <td>3.009371e-05</td>\n",
       "      <td>4.805267e-05</td>\n",
       "      <td>1.049549e-02</td>\n",
       "      <td>0.180833</td>\n",
       "      <td>4.284928e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.232054e-03</td>\n",
       "      <td>1.310741e-03</td>\n",
       "      <td>8.574809e-03</td>\n",
       "      <td>1.100413e-03</td>\n",
       "      <td>1.093662e-02</td>\n",
       "      <td>4.654031e-04</td>\n",
       "      <td>0.991535</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>1.238674e-04</td>\n",
       "      <td>4.009687e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>2.869463e-07</td>\n",
       "      <td>2.575255e-07</td>\n",
       "      <td>0.229427</td>\n",
       "      <td>3.598879e-07</td>\n",
       "      <td>9.993396e-01</td>\n",
       "      <td>2.019457e-09</td>\n",
       "      <td>5.498687e-08</td>\n",
       "      <td>0.759775</td>\n",
       "      <td>5.472131e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.209471e-07</td>\n",
       "      <td>1.903473e-06</td>\n",
       "      <td>6.965385e-07</td>\n",
       "      <td>2.022771e-03</td>\n",
       "      <td>1.044015e-04</td>\n",
       "      <td>3.732358e-07</td>\n",
       "      <td>0.490771</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>8.786441e-08</td>\n",
       "      <td>9.284461e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43660</th>\n",
       "      <td>TEST_43660</td>\n",
       "      <td>5.156089e-12</td>\n",
       "      <td>3.324739e-11</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>1.145958e-17</td>\n",
       "      <td>3.801710e-06</td>\n",
       "      <td>2.568055e-13</td>\n",
       "      <td>6.674786e-10</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>9.971169e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.497656e-07</td>\n",
       "      <td>1.278369e-05</td>\n",
       "      <td>1.070977e-07</td>\n",
       "      <td>1.341101e-06</td>\n",
       "      <td>6.215746e-06</td>\n",
       "      <td>8.092380e-08</td>\n",
       "      <td>0.902448</td>\n",
       "      <td>0.035549</td>\n",
       "      <td>1.080540e-08</td>\n",
       "      <td>9.642080e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43661</th>\n",
       "      <td>TEST_43661</td>\n",
       "      <td>1.480062e-08</td>\n",
       "      <td>7.016538e-09</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>5.976247e-10</td>\n",
       "      <td>9.999002e-01</td>\n",
       "      <td>1.171631e-09</td>\n",
       "      <td>1.482069e-10</td>\n",
       "      <td>0.141470</td>\n",
       "      <td>1.942284e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.723258e-08</td>\n",
       "      <td>1.322393e-06</td>\n",
       "      <td>1.438852e-07</td>\n",
       "      <td>1.974784e-06</td>\n",
       "      <td>2.228372e-05</td>\n",
       "      <td>2.187507e-07</td>\n",
       "      <td>0.356037</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>5.602574e-08</td>\n",
       "      <td>2.636658e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43662</th>\n",
       "      <td>TEST_43662</td>\n",
       "      <td>5.235693e-04</td>\n",
       "      <td>3.095625e-04</td>\n",
       "      <td>0.739654</td>\n",
       "      <td>5.993159e-04</td>\n",
       "      <td>8.916872e-05</td>\n",
       "      <td>1.595895e-05</td>\n",
       "      <td>2.271070e-05</td>\n",
       "      <td>0.417297</td>\n",
       "      <td>1.112014e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.879758e-05</td>\n",
       "      <td>1.803340e-04</td>\n",
       "      <td>1.058874e-03</td>\n",
       "      <td>2.326487e-02</td>\n",
       "      <td>4.522066e-01</td>\n",
       "      <td>6.478430e-05</td>\n",
       "      <td>0.658082</td>\n",
       "      <td>0.153801</td>\n",
       "      <td>5.228743e-04</td>\n",
       "      <td>3.575902e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43663</th>\n",
       "      <td>TEST_43663</td>\n",
       "      <td>1.806959e-05</td>\n",
       "      <td>3.588037e-04</td>\n",
       "      <td>0.908139</td>\n",
       "      <td>2.998814e-07</td>\n",
       "      <td>7.276814e-07</td>\n",
       "      <td>1.428136e-07</td>\n",
       "      <td>1.997791e-03</td>\n",
       "      <td>0.558091</td>\n",
       "      <td>4.642875e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.779907e-07</td>\n",
       "      <td>6.446212e-08</td>\n",
       "      <td>5.320634e-07</td>\n",
       "      <td>2.754557e-05</td>\n",
       "      <td>1.205022e-01</td>\n",
       "      <td>3.233155e-06</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.289396</td>\n",
       "      <td>1.688543e-03</td>\n",
       "      <td>9.951506e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43664</th>\n",
       "      <td>TEST_43664</td>\n",
       "      <td>1.320761e-07</td>\n",
       "      <td>1.238229e-08</td>\n",
       "      <td>0.488968</td>\n",
       "      <td>3.538795e-15</td>\n",
       "      <td>1.903168e-08</td>\n",
       "      <td>1.183838e-09</td>\n",
       "      <td>1.496427e-06</td>\n",
       "      <td>0.038220</td>\n",
       "      <td>9.557426e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237972e-06</td>\n",
       "      <td>1.830461e-05</td>\n",
       "      <td>4.379910e-06</td>\n",
       "      <td>6.224486e-07</td>\n",
       "      <td>5.496309e-06</td>\n",
       "      <td>1.587926e-05</td>\n",
       "      <td>0.993063</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.352719e-07</td>\n",
       "      <td>4.033526e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43665 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_id      airplane       airport  bare soil  baseball diamond  \\\n",
       "0      TEST_00000  7.122533e-17  1.252369e-15   0.000013      1.377356e-26   \n",
       "1      TEST_00001  2.042777e-10  1.642777e-10   0.861272      3.610178e-21   \n",
       "2      TEST_00002  4.444573e-07  6.065267e-10   0.678852      3.712249e-15   \n",
       "3      TEST_00003  2.090319e-05  5.493608e-05   0.627948      2.127815e-09   \n",
       "4      TEST_00004  2.869463e-07  2.575255e-07   0.229427      3.598879e-07   \n",
       "...           ...           ...           ...        ...               ...   \n",
       "43660  TEST_43660  5.156089e-12  3.324739e-11   0.000034      1.145958e-17   \n",
       "43661  TEST_43661  1.480062e-08  7.016538e-09   0.003190      5.976247e-10   \n",
       "43662  TEST_43662  5.235693e-04  3.095625e-04   0.739654      5.993159e-04   \n",
       "43663  TEST_43663  1.806959e-05  3.588037e-04   0.908139      2.998814e-07   \n",
       "43664  TEST_43664  1.320761e-07  1.238229e-08   0.488968      3.538795e-15   \n",
       "\n",
       "       basketball court         beach        bridge  buildings          cars  \\\n",
       "0          3.027507e-13  5.802833e-12  2.288399e-18   0.000231  9.502489e-09   \n",
       "1          1.523815e-11  3.414202e-14  5.685124e-17   1.000000  9.998770e-01   \n",
       "2          1.368452e-09  2.161503e-10  6.784780e-07   0.049860  9.284833e-01   \n",
       "3          3.009371e-05  4.805267e-05  1.049549e-02   0.180833  4.284928e-01   \n",
       "4          9.993396e-01  2.019457e-09  5.498687e-08   0.759775  5.472131e-01   \n",
       "...                 ...           ...           ...        ...           ...   \n",
       "43660      3.801710e-06  2.568055e-13  6.674786e-10   0.999947  9.971169e-01   \n",
       "43661      9.999002e-01  1.171631e-09  1.482069e-10   0.141470  1.942284e-02   \n",
       "43662      8.916872e-05  1.595895e-05  2.271070e-05   0.417297  1.112014e-01   \n",
       "43663      7.276814e-07  1.428136e-07  1.997791e-03   0.558091  4.642875e-02   \n",
       "43664      1.903168e-08  1.183838e-09  1.496427e-06   0.038220  9.557426e-01   \n",
       "\n",
       "       ...         tanks  tennis court       terrace         track  \\\n",
       "0      ...  4.368716e-13  1.307474e-11  5.247762e-09  1.958257e-14   \n",
       "1      ...  2.435036e-11  3.603454e-11  1.638018e-10  1.539852e-11   \n",
       "2      ...  4.811736e-08  6.522838e-07  2.212666e-07  1.565127e-08   \n",
       "3      ...  1.232054e-03  1.310741e-03  8.574809e-03  1.100413e-03   \n",
       "4      ...  3.209471e-07  1.903473e-06  6.965385e-07  2.022771e-03   \n",
       "...    ...           ...           ...           ...           ...   \n",
       "43660  ...  2.497656e-07  1.278369e-05  1.070977e-07  1.341101e-06   \n",
       "43661  ...  2.723258e-08  1.322393e-06  1.438852e-07  1.974784e-06   \n",
       "43662  ...  4.879758e-05  1.803340e-04  1.058874e-03  2.326487e-02   \n",
       "43663  ...  3.779907e-07  6.446212e-08  5.320634e-07  2.754557e-05   \n",
       "43664  ...  1.237972e-06  1.830461e-05  4.379910e-06  6.224486e-07   \n",
       "\n",
       "              trail  transmission tower     trees     water       wetland  \\\n",
       "0      3.864470e-07        1.603567e-08  0.002248  0.000011  7.547775e-09   \n",
       "1      1.611498e-09        1.255012e-10  0.999877  0.005178  4.689848e-12   \n",
       "2      2.063420e-05        6.402930e-07  0.301665  0.000005  1.140984e-08   \n",
       "3      1.093662e-02        4.654031e-04  0.991535  0.051700  1.238674e-04   \n",
       "4      1.044015e-04        3.732358e-07  0.490771  0.000064  8.786441e-08   \n",
       "...             ...                 ...       ...       ...           ...   \n",
       "43660  6.215746e-06        8.092380e-08  0.902448  0.035549  1.080540e-08   \n",
       "43661  2.228372e-05        2.187507e-07  0.356037  0.000006  5.602574e-08   \n",
       "43662  4.522066e-01        6.478430e-05  0.658082  0.153801  5.228743e-04   \n",
       "43663  1.205022e-01        3.233155e-06  0.008879  0.289396  1.688543e-03   \n",
       "43664  5.496309e-06        1.587926e-05  0.993063  0.000023  1.352719e-07   \n",
       "\n",
       "       wind turbine  \n",
       "0      8.717879e-20  \n",
       "1      1.003897e-26  \n",
       "2      1.851786e-14  \n",
       "3      4.009687e-09  \n",
       "4      9.284461e-11  \n",
       "...             ...  \n",
       "43660  9.642080e-21  \n",
       "43661  2.636658e-16  \n",
       "43662  3.575902e-08  \n",
       "43663  9.951506e-01  \n",
       "43664  4.033526e-17  \n",
       "\n",
       "[43665 rows x 61 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_columns = [col for col in df_train.columns if col not in ['img_id', 'img_path']]\n",
    "# df_predictions[class_columns] = df_predictions[class_columns].astype(int)\n",
    "# df_predictions[class_columns] = df_predictions[class_columns].round(2)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "77c29f42-3ec8-4ceb-be5a-72723d90f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
